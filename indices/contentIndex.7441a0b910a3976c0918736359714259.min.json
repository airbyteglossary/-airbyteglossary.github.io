{"/":{"title":"Data Glossary 🧠","content":"\n## A Single Place for All Data Knowledge\nFind all Glossary Terms below:\n* [By Tags](tags)\n\nIf you just want to get inspired, you can start with one of these terms:\n- [Data Engineering](term/data%20engineering.md) // [Business Intelligence](term/business%20intelligence.md) // [Analytics](term/analytics.md) // [Machine Learning](term/machine%20learning.md)\n- [Data Engineering Concepts](term/data%20engineering%20concepts.md) // [Data Engineering Guides](term/data%20engineering%20guides.md) \n- [Data Lake](term/data%20lake.md) // [Data Lake Table Format](term/data%20lake%20table%20format.md) // [Data Lakehouse](term/data%20lakehouse.md)\n- [Programming Languages](term/programming%20languages.md) // [Functional Programming](term/functional%20programming.md)  // [Functional Data Engineering](term/functional%20data%20engineering.md)\n- [Airbyte Related Terms](tags/airbyte/)\n\nOtherwise, you can simply [Search](https://glossary.airbyte.com/#navigation) (`cmd/ctrl+k`) for terms and content. Or find out more [About this Glossary](term/about%20this%20glossary.md).\n\n\u003cbr\u003e\n\n\u003e [!info] How to Contribute?\n\u003e \n\u003e 1.  ⭐ Star our [GitHub](https://github.com/airbytehq/glossary) repo\n\u003e 2.  🗣️  [Share the Glossary](https://twitter.com/intent/tweet?text=Great%20definitions%20on%20the%20data%20glossary%20🧠%20by%20@airbytehq\u0026url=glossary.airbyte.com)\n\u003e 3.  ✍️ Missing a Term or want to fix a typo? [Contribute to Glossary](term/contribute%20to%20glossary.md) \n\u003e 4. 👀 Want to discuss or need help, talk to us on [Slack](https://slack.airbyte.com)\n\n\n","lastmodified":"2022-10-21T08:35:58.287162017Z","tags":null},"/term/about-this-glossary":{"title":"About this Glossary","content":"The Airbyte Glossary is built on top of the [Digital Garden](https://jzhao.xyz/posts/networked-thought/) analogy. Instead of aligning all glossary terms in a single level, the digital garden approach lets you go inwards. You can learn about each term and go deeper into each of its connections. The Glossary will show you each link that is related to the above interactive graph to it and all backlinks.\n\nThese will allow you to see connections in a visual way, that you would not otherwise.\n\nThis Glossary is forked from [Quartz](https://github.com/jackyzha0/quartz) and we thank Jacky for open-sourcing this gem.\n\n### Navigation\nYou can simply hit `ctrl/cmd+k` and **search** the whole Data Brain. Or you can click on the links and navigation through our content.\n\n### Interactive Graph\nUse the `Interactive Graph` on the bottom. It will appear every term. You can zoom and click on different nodes to navigate through the content.\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/acid-transactions":{"title":"What are ACID Transactions?","content":"An ACID transaction secures that either all changes are successfully committed or rollbacked. It makes sure you never end in an inconsistent state. There is different concurrency control that, for example, guarantees consistency between reads and writes. Each [Data Lake Table Format](term/data%20lake%20table%20format.md) has other implementations and features here.","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/airbyte-catalog":{"title":"Airbyte Catalog","content":"\n\u003e [!info] Info\n\u003e \n\u003e This is only relevant for individuals who want to create a connector.\n\nThis refers to how you define the data that you can retrieve from a Source. For example, if you want to retrieve information from an API, the data that you can receive needs to be defined clearly so that Airbyte can have a clear expectation of what endpoints are supported and what the objects that the streams return look like. This is represented as a sort of schema that Airbyte can interpret. \n\nLearn more on [Beginners Guide to Catalog](https://docs.airbyte.com/understanding-airbyte/beginners-guide-to-catalog).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/airbyte-cdk":{"title":"Airbyte CDK","content":"\nThe Airbyte CDK (Connector Development Kit) allows you to create connectors for Sources or Destinations. If your source or destination doesn't exist, you can use the CDK to make the building process a lot easier. It generates all the tests and files you need and all you need to do is write the connector-specific code for your source or destination. \n\nAn extensive [Step-by-Step Example](https://airbyte.com/tutorials/extract-data-from-the-webflow-api) of how to create a custom Airbyte source connector with the [Python CDK](https://docs.airbyte.com/connector-development/cdk-python/). Another example by the Faros AI team created with [Javascript/Typescript](https://docs.airbyte.com/connector-development/cdk-faros-js). ","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/airbyte-glossary-of-terms":{"title":"Glossary of Terms (Airbyte Specific)","content":"\nThis is the start of the Glossary relevant for [Airbyte](https://airbyte.com) specific, which are related to [docs.airbyte.com](https://docs.airbyte.com/) or when using Airbyte.\n\nYou'll find all terms at [#airbyte](/tags/airbyte/), or you can get inspired with the following terms:\n- [Airbyte CDK](term/airbyte%20cdk.md)\n- Related to [Incremental Sync](term/incremental%20synchronization.md) and [Full Refresh Sync](term/full%20refresh%20synchronization.md):\n\t- [Cursor](term/cursor.md), [Soft Delete](term/soft%20delete.md), [Partial Success](term/partial%20success.md), and  [Raw Tables](term/raw%20tables.md)\n- [Airbyte Normalization](term/normalization.md)\n- [ETL and ELT](term/etl%20elt%20airbyte.md)\n\n## Advanced Terms\n- [Airbyte Catalog](term/airbyte%20catalog.md)\n- [Airbyte Specification](term/airbyte%20specification.md)\n- [Temporal](term/temporal.md)","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/airbyte-specification":{"title":"Airbyte Specification","content":"\u003e [!info] Info\n\u003e \n\u003e This is only relevant for individuals who want to create a connector.\n\nThis refers to the functions that a Source or Destination must implement to successfully retrieve data and load it, respectively. Implementing these functions using the Airbyte Specification makes a Source or Destination work correctly. \n\nLearn more on [Airbyte Protocol](https://docs.airbyte.com/understanding-airbyte/airbyte-protocol).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/airbyte-streams":{"title":"What are Airbyte Streams?","content":"In order to understand **AirbyteStreams**, let’s first talk about the **AirbyteCatalog**. An **AirbyteCatalog** describes the structure of data in a data source. It has a single field called streams that contains a list of **AirbyteStreams**. Each **AirbyteStream** contains a _name_ and _json_schema_ field. The _json_schema_ field describes the structure of a stream. This data model is intentionally flexible.\n\nIf we are using a data source that is a traditional relational database, each table in that database would map to an **AirbyteStream**. Each column in the table would be a key in the _properties_ field of the _json_schema_ field.\n\nIf we are using a data source that wraps an API with multiple different resources (e.g. _api/customers_ and _api/products_) each route would correspond to a stream.","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/analytics":{"title":"What is Analytics?","content":"Analytics is the systematic computational analysis of [[data]] or statistics. It is used for the discovery, interpretation, and communication of meaningful patterns in data. It also entails applying data patterns toward effective decision-making.\n\nIt's highly related to [Business Intelligence](term/business%20intelligence.md).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/apache-arrow":{"title":"What is Apache Arrow?","content":"Apache Arrow is a development platform for in-memory analytics. It contains a set of technologies that enable big data systems to process and move data fast.\n\nRead more on [Data Lake File Format](term/data%20lake%20file%20format.md).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/apache-avro":{"title":"What is Apache Avro?","content":"Avro is an open-source data serialization system that helps with data exchange between systems, [programming languages](term/programming%20languages.md), and processing frameworks. Avro helps define a binary format for your data, as well as map it to the programming language of your choice.\n\nAvro has a JSON-like data model, but can be represented as either JSON or in a compact binary form. It comes with a **very sophisticated schema description language** that describes data. Avro is another [Data Lake File Format](term/data%20lake%20file%20format.md).\n\nRead more about how to build a Data Lake on top of it on our [Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/apache-hadoop":{"title":"What is Apache Hadoop?","content":"Apache Hadoop is a collection of open-source software utilities that facilitates using a network of many computers to solve problems involving massive amounts of data and computation. It provides a software framework for distributed storage and processing of big data using the [MapReduce](term/map%20reduce.md) programming model.\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/apache-hive":{"title":"What is Apache Hive?","content":"Apache Hive is a [Data Warehouse](term/data%20warehouse.md) software project built on top of [Apache Hadoop](term/apache%20hadoop.md) for providing data queries and analysis. Hive gives an SQL-like interface to query data stored in various databases and file systems that integrate with Hadoop. Traditional SQL queries must be implemented in the [MapReduce](term/map%20reduce.md) Java API to execute SQL applications and queries over distributed data. Hive provides the necessary SQL abstraction to integrate SQL-like queries ([HiveQL](https://en.wikipedia.org/wiki/Apache_Hive#HiveQL)) into the underlying Java without the need to implement queries in the low-level Java API.","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/apache-hudi":{"title":"What is Apache Hudi?","content":"Apache Hudi is a [Data Lake Table Format](term/data%20lake%20table%20format.md) and was originally developed at Uber in 2016 (code-named and pronounced \"Hoodie\"), open-sourced end of 2016 ([first commit](https://github.com/apache/hudi/commit/0512da094bad2f3bcd2ddddc29e8abfec175dcfe) in 2016-12-16), and submitted to the Apache Incubator in January 2019. More about the back story on [The Apache Software Foundation Announces Apache® Hudi™ as a Top-Level Project](https://www.globenewswire.com/news-release/2020/06/04/2043732/0/en/The-Apache-Software-Foundation-Announces-Apache-Hudi-as-a-Top-Level-Project.html).\n\nRead more about how to build a Data Lake on top of it on our [Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/apache-iceberg":{"title":"What is Apache Iceberg?","content":"Apache Iceberg is a [Data Lake Table Format](term/data%20lake%20table%20format.md) and was [initially developed](https://github.com/Netflix/iceberg) at Netflix to solve long-standing issues using huge, petabyte-scale tables. It was open-sourced in 2018 as an Apache Incubator project and graduated from the incubator on the 19th of May 2020. Their [first public commit](https://github.com/apache/iceberg/commit/a5eb3f6ba171ecfc517a4f09ae9654e7d8ae0291) was 2017-12-19—more insights about the story on [A Short Introduction to Apache Iceberg](https://medium.com/expedia-group-tech/a-short-introduction-to-apache-iceberg-d34f628b6799).\n\nRead more about how to build a Data Lake on top of it on our [Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/apache-parquet":{"title":"What is Apache Parquet?","content":"Apache Parquet is a free and open-source column-oriented [Data Lake File Format](term/data%20lake%20file%20format.md) in the Apache Hadoop ecosystem. It is similar to RCFile and [ORC](term/orc.md), the other columnar-storage file formats in Hadoop, and is compatible with most of the data processing frameworks around Hadoop.\n\nRead more about how to build a Data Lake on top of it on our [Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/apache-spark":{"title":"What is Apache Spark?","content":"Apache Spark™ is an open-source multi-language engine for executing [Data Engineering](term/data%20engineering.md)  and [Machine Learning](term/machine%20learning.md) on single-node machines or clusters. It's optimized for large-scale data processing.\n\nSpark runs well with [Kubernetes](term/kubernetes.md).\n\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/bus-matrix":{"title":"What is Bus Matrix?","content":"A Bus Matrix was traditionally used in [OLAP](term/olap%20(online%20analytical%20processing).md) cubes such as Microsoft SSAS and co. They let you visually see what [[Measure]] can be queried with which [dimensions](term/dimensions.md).\n\nThey look something like this, example of [SSAS](https://blog.exsilio.com/all/ssas-dimensions-and-cube-basics/):\n![](https://lh3.googleusercontent.com/rtcy8qRao3f_3dXYyFRRRhE2Q21stS9gITYq4YJh2Y3iYf4QUYJgPGWehqwZmryWLfZARniGvboL_aeLwAblhxmClk4rj418Jof1ijdjocu61shPJzu1KdTk4UWxZWAToqgz8aVIiQXcHXWTc9I7yQoVtASNC3GQjcOTkKxAPehuFSHQdR1wOhbUPA)\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/business-intelligence":{"title":"What is Business Intelligence?","content":"Business intelligence (BI) leverages software and services to transform data into actionable insights that inform an organization’s business decisions. The new term is [Data Engineering](term/data%20engineering.md). The language of a BI engineer is [SQL](term/sql.md).\n\n## Goals of BI\nBI should produce a simple overview of your business, boost efficiency, and automate repetitive tasks across your organization. In more detail:\n  * **Roll-up capability** - (data) [Visualization](term/analytics.md) over the most important [KPIs][2] (aggregations) - like a cockpit in an airplane which gives you the important information at one glance.\n  * **Drill-down possibilities** - from the above high-level overview drill down the very details to figure out why something is not performing as planned. **Slice-and-dice or pivot your data from different angles.\n  * **Single source of truth** - instead of multiple spreadsheets or other tools with different numbers, the process is automated and done for all unified. Employees can talk about the business problem instead of the various numbers everyone has. Reporting, budgeting, and forecasting are automatically updated and consistent, accurate, and in timely manner.\n  * **Empower users**: With the so-called self-service BI, every user can analyze their data instead of only BI or IT persons.\n\nRead more on [Business Intelligence meets Data Engineering with Emerging Technologies](https://www.sspaeti.com/blog/business-intelligence-meets-data-engineering/).\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/business-intelligence-tools":{"title":"What are Business Intelligence Tools?","content":"\n[Business Intelligence](term/business%20intelligence.md) tools visualizing your data across the organizations. See a curated list of tools including the referenced image below on [Catalog of BI tools](https://notion.castordoc.com/catalog-of-bi-tools).\n\n![](images/business-intelligence-tools-landscape.png)\n\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/cdp-customer-data-platform":{"title":"What is a CDP (Customer Data Platform)?","content":"\nA customer Data Platform (CDP) is a system that collects large quantities of customer data (i.e. information about your customers) from a variety of channels and devices, helping to make this data more accessible to the people who need it. CDPs are responsible for sorting and categorizing data, as well as data cleansing to remove inaccurate or out-of-date information.\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/contribute-to-glossary":{"title":"How to Contribute to the Glossary","content":"\n\u003e [!info] General Infos\n\u003e \n\u003e If you want to know more general how this glossary works, see [General Info](term/general%20infos.md) with a description of folder structure, how to create a link, etc.\n\nDeployment wise you have three options, which are explained in the chapters below:\n * Web edits through GitHub\n* Creating an Issue and we'll do the rest\n* Or cloning and running it locally\n\n## Web Edit with GitHub\nYou can either click on `Edit Source` on each page and directly edit on GitHub or you can create a [New Issue](https://github.com/airbytehq/glossary/issues).\n\n## Create an Issue\nIf you are unsure, you can always [create an issue](https://github.com/airbytehq/glossary/issues) on our GitHub repo and we will make sure to add the new entry.\n\n## Changing a lot? Clone locally\n### Clone it locally with git\nClone the [repo](https://github.com/airbytehq/glossary) with:\n```sh\ngit clone https://github.com/airbytehq/glossary.git\n```\n\n### Editors\n#### Obsidian as an Editor (Recommended)\nIf you want to use [Obsidian](https://obsidian.md/), which I recommend as it will handle all links when renaming terms, adding a nice Markdown view with lots of features (even if you don't need them) and showing backlinks and [graph](term/about%20this%20glossary.md#interactive-graph). Just open the Obsidian in the folder `content/`, there is a hidden folder called `.obsidian` which does the rest.\n\n1. ![](images/setup-obsidian-vault.png)\n2. ![](images/setup-folder-structure.png)\n\nMore details and step-by-step manual you see on [Quartz Setup](https://quartz.jzhao.xyz/notes/setup/), how to [Edit Notes ](https://quartz.jzhao.xyz/notes/editing/) and [How to set up Obsidian](https://quartz.jzhao.xyz/notes/obsidian/) (although the settings are already done when you open the `.obsidian` folder as described above).\n\n#### Use any other Editor\nOf course, as everything is Markdown, you can edit each file under `content/term` as a normal markdown file and publish (see below) changes to GitHub. Keep in mind, ethat very new term you create will automatically be created as a page in dthe eployment process or when you run it locally.\n\n### Preview Locally\n#### Setup\nQuartz runs on top of [Hugo](https://gohugo.io/) so all notes are written in [Markdown](https://www.markdownguide.org/getting-started/).\n\nWe need to install golang, hugo-obsidian and hugo. Follow the instructions on [Preview Changes on Quartz](https://quartz.jzhao.xyz/notes/preview-changes/).\n\n\u003e [!info]\n\u003e \n\u003e If you are running into an error saying that `command not found: hugo-obsidian`, make sure you set your `GOPATH` correctly! This will allow your terminal to correctly recognize hugo-obsidian as an executable.\n\nI added to my `~/.zshrc` (or `~/.bashrc`):\n ```sh\n#go path\nexport GOPATH=$HOME/go\nexport PATH=$PATH:$GOROOT/bin:$GOPATH/bin\n```\n#### Run it!\nAll you need to do it goint to your root directory of you cloned repo and start `make serve`:\n```sh\n# Navigate to your local Quartz folder\ncd \u003clocation-of-your-local-quartz\u003e\n\n# Start local server\nmake serve\n\n# View your site in a browser at http://localhost:1313/\n```\nThat's it, from now on that's how you run it. All changes you make will be automatically published, you do not need to stop and restart when you add terms, etc. (only the graph view will only be updated after stopping and serving again).\n\n## How to Publish\nCommit and Push to branch `hugo` and wait a couple of minutes until [GitHub Actions](https://github.com/airbytehq/glossary/actions) will deploy it automatically. At the moment we do not need to create PR's to make the updates as easy as possible. \n\nIf we encounter problems in the future, we might change that. If you are unsure, you can always [create an issue](https://github.com/airbytehq/glossary/issues) and we will make sure to add the new entry.\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/cte-common-table-expression":{"title":"What is a CTE (Common Table Expression)?","content":"\nA Common Table Expression (CTE) is a temporary named result set that you can reference within a SELECT, INSERT, UPDATE, or DELETE statement. The CTE can also be used in a View.\n\n```sql\nWITH cte_query AS\n(SELECT … subquery ...)\nSELECT main query ... FROM/JOIN with cte_query ...\n```\n\n## Types: Recursive and Non-Recursive\n### Non-Recursive CTE\nThere are two types of CTEs: Recursive and Non-Recursive.\n\nThe non-recursive are simple where CTE is used to avoid SQL duplication by referencing a name instead of the actual SQL statement.\n\nE.g.\n```sql\nWITH avg_per_store AS\n  (SELECT store, AVG(amount) AS average_order\n   FROM orders\n   GROUP BY store)\nSELECT o.id, o.store, o.amount, avg.average_order AS avg_for_store\nFROM orders o\nJOIN avg_per_store avg\nON o.store = avg.store;\n```\n\n### Recursive CTE\n\nRecursive CTEs use repeated procedural loops therefore the recursion. The recursive query calls itself until the query satisfied the condition. In a recursive CTE, we should provide a where condition to terminate the recursion.\n\nA recursive CTE is useful in querying hierarchical data such as organization charts where one employee reports to a manager or multi-level bill of materials when a product consists of many components, and each component itself also consists of many other components.\n\n```sql\nWITH levels AS (\n  SELECT\n    id,\n    first_name,\n    last_name,\n    superior_id,\n    1 AS level\n  FROM employees\n  WHERE superior_id IS NULL\n  UNION ALL\n  SELECT\n    employees.id,\n    employees.first_name,\n    employees.last_name,\n    employees.superior_id,\n    levels.level + 1\n  FROM employees, levels\n  WHERE employees.superior_id = levels.id\n)\n \nSELECT *\nFROM levels;\n```\n\nSee more on[5 Practical SQL CTE Examples | LearnSQL.com](https://learnsql.com/blog/practical-sql-cte-examples/).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/cursor":{"title":"What is a Cursor?","content":"At a conceptual level, a cursor is a tracker that is used during [incremental synchronization](term/incremental%20synchronization.md) to ensure that only newly updated or inserted records are sent from a data source to a destination in any given synchronization iteration.\n\nAirbyte’s incremental synchronization can be conceptually thought of as a loop which periodically executes synchronization operations. Each iteration of this loop only replicates records that have been inserted or updated in the source system since the previous execution of this synchronization loop – in other words, each synchronization operation will copy only records that have not previously been replicated by previous synchronizations. This is much more efficient than copying an entire dataset on each iteration, which is the behavior of full refresh synchronization.\n\nSending only updated or newly inserted documents requires tracking which records have already been replicated in previous synchronizations. This is done by a cursor, which can be thought of as a pointer to the most recent record that has been replicated by a given synchronization. When selecting documents for synchronization, Airbyte includes the most recent cursor value as part of the query on the source system to ensure that only new/updated records will be replicated.\n\nFor example, a source database could contain records which include a field called `updated_at`, which stores the most recent time that a record is inserted or updated. If `updated_at` is selected as the cursor field, then after a given synchronization operation the cursor will remember the largest `updated_at` value that has been seen in the records that have been replicated to the destination in that synchronization. In the subsequent synchronization operation, records that have been inserted or updated on the source are retrieved by including the cursor value as part of the query, so that it only selects records where the `updated_at` value is greater than (and in some edge cases greater than or equal to) the largest `updated_at` value seen in the previous synchronization.\n\nNote that while it is not strictly necessary to choose a time field for a cursor field, the field that is chosen should be monotonically increasing over time.\n\nRead more on [Incremental data synchronization between Postgres databases](https://airbyte.com/tutorials/incremental-data-synchronization). ","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/dag-directed-acyclic-graph":{"title":"What is a Directed Acyclic Graph (DAG)?","content":"DAG stands for **Directed Acyclic Graph**. A DAG is a graph where information must travel along with a finite set of nodes connected by vertices. There is no particular start or node and also no way for data to travel through the graph in a loop that circles back to the starting point.\n\nIt's a popular way of building data pipelines in tools like [[Airflow]], [[Dagster]], [[Prefect]]. It clearly defines the [Data Lineage](term/data%20lineage.md). As well, it's made for a functional approach where you have the [idempotency](term/idempotency.md) to restart pipelines without side-effects.\n\n![](images/dag.png)","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/data-asset":{"title":"What is a Data Asset?","content":"A data asset is typically a database table, a machine learning model, or a report. A persistent object that captures some understanding of the world. It's more a technical term where [Data Product](term/data%20product.md) is more used in general or in [Data Mesh](term/data%20mesh.md).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/data-catalog":{"title":"What is a Data Catalog?","content":"A Data Catalog is a centralized store where all metadata data about your data is made searchable.\n\n**Think about a Google Search for your internal Metadata**. This is vital, as with [Data Lake](term/data%20lake.md) and other data stores, and you want the ability to search for your data. Data is growing exponentially, with 90% of the world’s data being generated alone in the last two years. It's hard to keep this amount over time. A data catalog solves the problem of the fast-growing handling of data internally.\n\nAn interesting read about the beginning of the Data Catalog is explained in the 2017 published paper about a [Data Context Service](http://cidrdb.org/cidr2017/papers/p111-hellerstein-cidr17.pdf).  \n\nSee a High-Level Feature Comparison by the [Awesome Data Discovery and Observability](https://github.com/opendatadiscovery/awesome-data-catalogs) list on GitHub (check out the link for more):\n![](images/data-catalog-feature-comparison2.png)\n\nOr a great overview by Sarah Krasnik on [Choosing a Data Catalog](https://sarahsnewsletter.substack.com/p/choosing-a-data-catalog):\n![](images/data-catalog-overview-sarah.png)\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/data-contract":{"title":"What is a Data Contract?","content":"\nData Contracts are API-like agreements between software/data engineers who own services and data consumers that understand how the business works. The goal is to generate well-modeled, high-quality, trusted, real-time data.\n\nIt's an **abstraction** that allows engineers to decouple their databases and services from analytics and ML requirements. It will avoid production-breaking incidents when modifying the schema as they are validated and enforced.\n\n![](images/data-contract.png)\nIllustration by Chad Sanderson on [The Rise of Data Contracts - by Chad Sanderson](https://dataproducts.substack.com/p/the-rise-of-data-contracts)\n\n[Chad Sanderson](https://www.linkedin.com/in/chad-sanderson/) said that at Convoy, they use [[Protobuf]] and [[Apache Kafka]] to abstract the CRUD transactions. They define the schema based on what they *need*, not what they get from the source. Same as [[Software-Defined Assets]] describe the [Data Asset](term/data%20asset.md) in a declarative manner and set [expectations](https://github.com/dagster-io/dagster/discussions/9543).\n\nConfluent also built similar functions on top of Kafka with their [Schema Registry](https://docs.confluent.io/platform/current/schema-registry/), and terms such as [Semantic Layer](term/metrics%20layer.md) and [Analytics API](https://www.sspaeti.com/blog/analytics-api-with-graphql-the-next-level-of-data-engineering/#what-is-an-analytics-api) (with [[GraphQL]]) are trying to achieve similar things.\n\nData Contracts are not meant to replace data pipelines and [Modern Data Stack](term/modern%20data%20stack.md), a more batch approach. These are good for fast prototyping. You could start defining data contracts when you have some knowledge about data.\n\nInterestingly, the differentiation to [Data Mesh](term/data%20mesh.md) is an organizational framework with a micro-service approach to data. Data Mesh doesn't inform which data should be emitted or validate the data being emitted from production is correct or conforms to a consumer's expectations.\n\nAlso, data contracts are a form of [Data Governance](term/data%20governance.md). This term is very vague and gets more concrete with explicit contracts. You can also use [Great Expectations](https://greatexpectations.io/) to set expectations for your data, which I believe is a great way to start.\n\n## From the Discussion on YouTube w/ Chad Sanderson vs Ethan Aaron\n\nChad Sanderson says in [Data Contract Battle Royale w/ Chad Sanderson vs Ethan Aaron - YouTube](https://youtu.be/4BEpYAp3Qu4) :\n- It's just a database version of a real-world contract. \n- A real-world contract is just an agreement between two parties where:\n\t- There's some mechanism for enforcing that it happens. \n\t- A data contract is a similar agreement, but it's **between someone that produces data and consumes data** to vend a particular data set which usually includes a schema and some enforcement mechanism. \n- Differentiation between data contract and data product:\n\t- **Data contract**, which is *what* is the data and *how* do we enforce this quality \n\t- **[Data Product](term/data%20product.md)** which is *why* do we need this data \n\nEthan Aaron is saying his problem with data contracts is that you focus on defining the interface/contract too early. E.g., if you have a big task done by several teams or people, you have a contract to agree on an interface. I'd argue that's precisely what the data products are, and instead of agreeing on some artificial contract, decide on the product, so the tools and teams can be distinct.\n\n## Summary Blog Posts\nAn excellent summary by [Mehdi Ouazza](https://www.linkedin.com/in/mehd-io) about data contracts [From Zero To Hero](https://towardsdatascience.com/data-contracts-from-zero-to-hero-343717ac4d5e). He is illustrating how [[Apache Kafka]] could also be the interface that defines the contract.\n\n![](images/data-contract-example.png)\nIllustration from [Data Contracts — From Zero To Hero](https://towardsdatascience.com/data-contracts-from-zero-to-hero-343717ac4d5e)\n\nSee also [Semantic Warehouse](term/semantic%20warehouse.md).\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/data-engineering":{"title":"What is Data Engineering?","content":"The definition from the [Fundamentals of Data Engineering](https://www.oreilly.com/library/view/fundamentals-of-data/9781098108298/), as it’s one of the most recent and complete: \n\u003e Data engineering is the development, implementation, and maintenance of systems and processes that take in raw data and produce high-quality, consistent information that supports downstream use cases, such as analysis and machine learning. Data engineering intersects security, data management, DataOps, data architecture, orchestration, and software engineering.\n\nA data engineer today oversees the whole data engineering process, from collecting data from various sources to making it available for downstream processes. The role requires familiarity with the multiple stages of the data engineering lifecycle and an aptitude for evaluating data tools for optimal performance across several dimensions, including price, speed, flexibility, scalability, simplicity, reusability, and interoperability.\n\nData Engineering helps also overcome the bottlenecks of [Business Intelligence](term/business%20intelligence.md):\n- More transparency as tools are open-source mostly\n- More frequent data loads\n- Supporting [Machine Learning](term/machine%20learning.md) capabilities \n\nCompared to existing roles it would be a **software engineering plus business intelligence engineer including big data abilities** as the [Hadoop](term/apache%20hadoop.md) ecosystem, streaming, and computation at scale. Business creates more reporting artifacts themselves but with more data that needs to be collected, cleaned, and updated near real-time and complexity is expanding every day.\n\nWith that said more programmatic skills are needed similar to software engineering. **The emerging language at the moment is [Python](term/python.md)** which is used in engineering with tools alike [[Apache Airflow]], [Dagster](Dagster), [[Prefect]] as well as data science with powerful libraries.\n\nAs a data engineer, you use mainly [SQL](term/sql.md) for almost everything except when using external data from an API. Here you'd use [ELT](term/elt.md) tools or write some [[data pipelines]] with the tools mentioned above.\n\nWant to know more about [The Evolution of The Data Engineer: A Look at The Past, Present \u0026 Future](https://airbyte.com/blog/data-engineering-past-present-and-future), check out the linked article or watch the video form of it:\n{{\u003c youtube Si14Hgj4Lok \u003e}}\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/data-engineering-concepts":{"title":"Data Engineering Concepts","content":"Some core concepts we are going to explore:\n\n- [Data Warehouse](term/data%20warehouse.md) vs [Data Lake](term/data%20lake.md) vs [Data Lakehouse](term/data%20lakehouse.md)\n- [Storage Layer](term/storage%20layer%20object%20store.md) vs [Data Lake File Format](term/data%20lake%20file%20format.md) vs [Data Lake Table Format](term/data%20lake%20table%20format.md)\n- [Data Catalog](term/data%20catalog.md)\n- [Modern Data Stack](term/modern%20data%20stack.md) \n- [ELT](term/elt.md) vs [ETL](term/etl.md) vs [EtLT](term/etlt.md)\n- [Functional Data Engineering](term/functional%20data%20engineering.md)\n- [Metrics Layer](term/metrics%20layer.md) vs [Semantic Warehouse](term/semantic%20warehouse.md) vs [Data Virtualization](term/data%20virtualization.md)\n\t- [Metrics](term/metric.md), [Key Performance Indicator (KPI)](term/key%20performance%20indicator%20(kpi).md)\n\t- [Push-Downs](term/push-down.md) vs [Rollup](term/rollup.md)\n\t- [Data Modeling](term/data%20modeling.md) vs [Dimensional Modeling](term/dimensional%20modeling.md)\n\t- [Data Contract](term/data%20contract.md)\n- [OLAP](term/olap%20(online%20analytical%20processing).md) vs [OLTP](term/oltp%20(online%20transactional%20processing).md)\n- [MapReduce](term/map%20reduce.md) and [Apache Hadoop](term/apache%20hadoop.md)\n- [Declarative vs Imperative](term/declarative.md)\n- [Notebooks](term/notebooks.md)\n\n\u003c!--\n- [[Batch processing]] vs [[Streaming Processing]]\n- [[Indexing]]\n- [[Relational Database]] vs [[NoSQL Database]]\n--\u003e\n\nSee also [What is Data Engineering](term/data%20engineering.md).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/data-engineering-guides":{"title":"Data Engineering Guides","content":"\nSome Data Engineering Guides that will help you learn [data engineering](term/data%20engineering.md):\n\n- **[Data Quality](https://airbyte.com/blog/data-quality-issues)**\n\t- How to handle [[term/data quality]] issues by detecting, understanding, fixing, and reduce\n- **[Data Lake / Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi)**\n\t- The what \u0026 why of a [Data Lake](term/data%20lake.md)\n\t- Differences between [Lakehouse](term/data%20lakehouse.md) \u0026 [Data Warehouse](term/data%20warehouse.md)\n\t- Components of a data lake\n\t\t1. [Storage Layer](term/storage%20layer%20object%20store.md)\n\t\t2. [Data Lake File Format](term/data%20lake%20file%20format.md)\n\t\t3. [Data Lake Table Format](term/data%20lake%20table%20format.md) with [Apache Parquet](term/apache%20parquet.md), [Apache Iceberg](term/apache%20iceberg.md), and [Apache Hudi](term/apache%20hudi.md)\n\t- Trends in the market\n\t- We answer questions such as:\n\t\t- How to build an open-source data lake offloading data for analytics?\n\t\t- How to [govern](term/data%20governance.md) your hundreds to thousands of files and have more database-like features?\n- **[Reverse ETL Explained](https://airbyte.com/blog/reverse-etl)**\n\t- A Brief Story of Data Integration: [ETL](term/etl.md) vs. [ELT](term/elt.md)\n\t- So, What is a [Reverse ETL](term/reverse%20etl.md)?\n\t- Technical Differences Between ETL and Reverse ETL\n\t- Typical Reverse ETL Use Cases\n\t- Reverse ETL and the [Data Hierarchy of Needs](term/data%20hierarchy%20of%20needs.md)\n- [Data Orchestration Trends](https://airbyte.com/blog/data-orchestration-trends)\n- [Data Integration Guide](https://airbyte.com/blog/data-integration)\n- [Understanding Change Data Capture (CDC)](https://airbyte.com/blog/change-data-capture-definition-methods-and-benefits)\n- [Using an ETL Framework vs Writing Yet Another ETL Script](https://airbyte.com/blog/etl-framework-vs-etl-script)\n\nSee more on [Data Insights Blog Posts](https://airbyte.com/blog-categories/data-insights).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/data-federation":{"title":"What is Data Federation","content":"\nData Federation is a virtual layer very similar to [Data Virtualization](term/data%20virtualization.md). The slight difference is that data federations include federated query engines such as [[Trino]], [[Presto]], [[Spark]], and alike. ","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/data-governance":{"title":"What is Data Governance?","content":"[**Data governance**](https://www.talend.com/resources/what-is-data-governance/) **is a collection of processes, roles, policies, standards, and metrics that ensure the effective and efficient use of information in enabling an organization to achieve its goals.** It establishes the processes and responsibilities that ensure the quality and security of the data used across a business or organization. Data governance defines who can take what action, upon what data, in what situations, and using what methods.\n\nRead more on our [Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/data-hierarchy-of-needs":{"title":"The Data Hierarchy of Needs","content":"\nThe data hierarchy of needs in this image is inspired by [Grouparoo's blog post](https://www.grouparoo.com/blog/data-hierarchy-of-needs):\n![](images/data-hierarchy-of-needs.png)\n\nMore on [Reverse ETL Explained](https://airbyte.com/blog/reverse-etl#so-what-is-a-reverse-etl).\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/data-lake":{"title":"What is a Data Lake?","content":"A Data Lake is a storage system with vast amounts of unstructured and structured data, stored as-is, without a specific purpose in mind, that can be built on multiple technologies such as Hadoop, NoSQL, Amazon Simple Storage Service, a relational database, or various combinations and different formats (e.g. Excel, CSV, Text, Logs, etc.).\n\nAccording to [Hortonworks Data Lake Whitepaper](http://hortonworks.com/wp-content/uploads/2014/05/TeradataHortonworks_Datalake_White-Paper_20140410.pdf), the data lake arose because new types of data needed to be captured and exploited by the enterprise. As this data became increasingly available, early adopters discovered that they could extract insight through new applications built to serve the business. The data lake supports the following capabilities:\n-   To capture and store raw data at scale for a low cost\n-   To store many types of data in the same repository\n-   To perform transformations on the data where the purpose may not be defined\n-   To perform new types of data processing\n-   To perform single-subject analytics based on particular use cases\n\nThe initial concept was created by Databricks in the [CIDR Paper](http://cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf) in 2021. Read more on our [Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/data-lake-file-format":{"title":"What is a Data Lake File Format?","content":"Data lake file formats are the new CSVs on the cloud. They are more column-oriented and compress large files with added features. The main players here are [Apache Parquet](term/apache%20parquet.md), [Apache Avro](term/apache%20avro.md), and [Apache Arrow](term/apache%20arrow.md). It’s the physical store with the actual files distributed around different buckets on your [Object Store](term/storage%20layer%20object%20store.md).\n\nYou can build more features with [Data Lake Table Format](term/data%20lake%20table%20format.md) on top. Read more on our [Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/data-lake-table-format":{"title":"What is a Data Lake Table Format?","content":"Data lake table formats are very attractive as they are databases on [Data Lake](term/data%20lake.md). Same as a table, one **data lake table format bundles distributed files into one table that is otherwise hard to manage**. You can think of it as an abstraction layer between your physical data files and how they are structured to form a table.\n\nIt is built on top o the [Storage Layer](term/storage%20layer%20object%20store.md) and [Data Lake File Format](term/data%20lake%20file%20format.md). Read more on our [Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/data-lake-transaction-log":{"title":"What is a Data Lake Transaction Log?","content":"The **data lake transaction log** is the ordered record of every transaction since its inception. A transaction log is a common component used through many of its above-mentioned features, including [ACID Transactions](term/acid%20transactions.md), scalable metadata handling, and [Time Travel](term/time%20travel.md). For example, [Delta Lake](term/delta%20lake.md) creates a single [folder called `_delta_log`](https://airbyte.com/tutorials/load-data-into-delta-lake-on-databricks-lakehouse#step-5).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/data-lakehouse":{"title":"What is a Data Lakehouse?","content":"\nA Data Lakehouse open data management architecture that combines the flexibility, cost-efficiency, and scale of [Data Lake](term/data%20lake.md) with the data management and ACID transactions of [Data Warehouse](term/data%20warehouse.md) with Data Lake Table Formats (Delta Lake, Apache Iceberg \u0026 Hudi) that enable Business Intelligence (BI) and Machine Learning (ML) on all data.\n\nThe initial concept was created by Databricks in the [CIDR Paper](http://cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf) in 2021. Read more on our [Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/data-lineage":{"title":"What is Data Lineage?","content":"Data lineage uncovers the life cycle of data. It aims to show the complete data flow from start to finish. Data lineage is the process of understanding, recording, and visualizing data as it flows from data sources to consumption. This includes all data transformations (what changed and why).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/data-mesh":{"title":"What is Data Mesh?","content":"The [Data Mesh Paper](https://martinfowler.com/articles/data-monolith-to-mesh.html) tries to eliminate silos between data teams, ensuring that the experience and knowledge about data are shared among all data consumers in the company. Data Mesh sees [Data as a Product](term/data%20product.md). Data meshes are also about connecting platforms that those teams are using so data can be easily moved around for the organization's benefit. Companies will try to find better ways of unifying and connecting the tools so that data professionals don’t have to switch and work in a silo.\n\nData meshes try to eliminate the tensions between decentralizing and centralizing data resources, with some common infrastructure but otherwise mostly decentralized. It empowers data teams and gives ownership to domain experts.\n\nMore valuable resources such as a [short version](https://cnr.sh/essays/what-the-heck-data-mesh), a [visually appealing one](https://www.datamesh-architecture.com/)), or [applied in practice](https://youtu.be/eiUhV56uVUc).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/data-modeling":{"title":"What is Data Modeling?","content":"\nData modeling in software engineering is the process of creating a data model for an information system by applying certain formal techniques. One specific example of data modeling is [Dimensional Modeling](term/dimensional%20modeling.md) which has a high state even in modern data architecture.\n\nRead more on [Wikipedia](https://en.wikipedia.org/wiki/Data_modeling).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/data-ops":{"title":"What is DataOps?","content":"Similar to how [DevOps](term/dev%20ops.md) changed the way software is developed, DataOps is changing the way data products are created. With DataOps, data engineers and data scientists can work together, bringing a level of collaboration and communication, with a common goal of producing valuable insight for the business.\n\n![](images/data-ops.png)\nRead more on [The Rise of DataOps](https://medium.com/towards-data-science/the-rise-of-dataops-2788958034ee).\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/data-orchestrator":{"title":"What is a Data Orchestrator?","content":"A Data Orchestrator models dependencies between different tasks in [complex heterogeneous cloud environments](https://mattturck.com/data2021/) end-to-end. It handles integrations with legacy systems, new cloud-based tools, and your data lakes and data warehouses. It invokes [computation](https://en.wikipedia.org/wiki/Orchestration_(computing)), such as wrangling your business logic in [SQL](term/sql.md) and [Python](term/python.md) and applying ML models at the right time based on a time-based trigger or by custom-defined logic.\n\nMore Insights in [Data Orchestration Trends: The Shift from Data Pipelines to Data Products](https://airbyte.com/blog/data-orchestration-trends).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/data-product":{"title":"What is a Data Product?","content":"[DJ Patil](https://twitter.com/dpatil), the former Chief Data Scientist of the United States, defined a data product as \"a product that facilitates an end goal through data.\" Also, [Data Mesh](term/data%20mesh.md) talks about \"data as a product.\" It applies more product thinking, whereas the \"Data Product\" essentially is a dashboard, report, and table in a [Data Warehouse](term/data%20warehouse.md) or a Machine Learning model. Sometimes Data Products are also called [Data Asset](term/data%20asset.md)s.","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/data-quality":{"title":"What is Data Quality?","content":"Data quality is the process of ensuring data meets expectations.\n\nThere are three main ways to detect a data quality issue: \n-   A business user reports an issue.\n-   A data test fails.\n-   Data monitoring raises an alert.\n\n![](images/data-quality.png)\n\nRead more on [Why is data quality harder than code quality?](https://airbyte.com/blog/data-quality-issues).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/data-swamp":{"title":"What is a Data Swamp?","content":"Data swamps start to arise when there is a lack of responsibilities, data ownership, availability, and data governance.  It's when a [Data Lake](term/data%20lake.md) is unmanaged or unable to provide value. Sometimes a Data Swamp can also arise from a [Data Warehouse](term/data%20warehouse.md) due to existing hybrid models.","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/data-virtualization":{"title":"What is Data Virtualization?","content":"Data Virtualization helps you when you have many source systems from different technologies, but all of them are rather fast in response time, and if you don't run a lot of operational applications. In that way, you don't move and copy data around and pre-aggregate, but you have a [Semantic Layer](term/metrics%20layer.md) where you create your business models (like cubes), and only if you query this data virtualization layer does it query the data source. If you use, e.g. [Dremio](https://www.dremio.com/), there you use [Apache Arrow](term/apache%20arrow.md) technology which will cache and optimize a lot in-memory for you that you have as well as stonishing fast response times.\n\nIt's tightly connected to [Data Federation](term/data%20federation.md) and [Push-Downs](term/push-down.md).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/data-warehouse":{"title":"What is a Data Warehouse?","content":"A Data Warehouse, in short DWH, also known as an Enterprise Data Warehouse (EDW), is the traditional way of collecting data as we do [since 30+ years](https://tdwi.org/articles/2016/02/01/data-warehousing-30.aspx). The DWH serves to be the data integration from many different sources, the single point of truth and the data management, meaning cleaning, historizing, and data joined together. It provides greater executive insight into corporate performance with management Dashboards, Reports, or Ad-Hoc Analyses.\n\nVarious types of business data are analyzed with Data Warehouses. The need for it often becomes evident when analytic requirements run afoul of the ongoing performance of operational databases. Running a complex query on a database requires the database to enter a temporarily fixed state. It is often untenable for transactional databases. A data warehouse is employed to do the analytical work, leaving the transactional database free to focus on transactions.\n\nThe other characteristic is analyzing data from multiple origins (e.g., your Google Analytics with your CRM data). It is highly transformed and structured due to the ETL (Extract Transform Load) process.\n\nIf you wonder about the difference between a Data Warehouse, Data Lake, and a Lakehouse, read more on our [Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/declarative":{"title":"What is declarative?","content":"A **[declarative](term/declarative.md)** data pipeline does not tell the order it needs to be executed but instead allows each step/task to find the best time and way to run. The declarative approach describes *what* the program does without explicitly specifying its control flow. [Functional Data Engineering](term/functional%20data%20engineering.md) and [Functional Programming](term/functional%20programming.md) is a **declarative** programming paradigm, in contrast to **[imperative](term/imperative.md)** programming paradigms.\n\n## Declarative vs Imperative\nDeclarative approaches appeal because they make systems easier to debug and automate. It's done by explicitly showing intention and offering a simple way to manage and apply changes. By explicitly declaring how the pipeline should look, for example, **defining the data products that should exist**, it becomes much easier to discover when it does not look like that, the reason why, and reconcile. It's the foundation layer for your entire platform's lineage, observability, and [data quality](https://airbyte.com/blog/data-quality-issues) monitoring.\n\n![](images/declarative-vs-imperative.png)\n\nRead more on [Data Orchestration Trends: The Shift From Data Pipelines to Data Products](https://airbyte.com/blog/data-orchestration-trends).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/delta-lake":{"title":"What is Delta Lake?","content":"Delta Lake is an open-source [Data Lake Table Format](term/data%20lake%20table%20format.md) project created by Databricks and kindly open-sourced with its [first public GitHub Commit](https://github.com/delta-io/delta/commit/14cb4e0267cc188e0fdd47e5b4f0235baf87874e) on 2019-04-22. Recently announced [Delta Lake 2.0](https://www.databricks.com/blog/2022/06/30/open-sourcing-all-of-delta-lake.html).\n\nRead more about how to build a Data Lake on top of it on our [Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi) or if you are curious to build a Delta Lake destination with Airbyte [Load Data into Delta Lake on Databricks Lakehouse](https://airbyte.com/tutorials/load-data-into-delta-lake-on-databricks-lakehouse).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/dev-ops":{"title":"What is DevOps?","content":"DevOps is a combination of software developers (dev) and operations (ops). It is defined as a software engineering methodology that aims to integrate the work of software development and software operations teams by facilitating a culture of collaboration and shared responsibility.\n\nIs also related to [DataOps](term/data%20ops.md)","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/dimensional-modeling":{"title":"What is Dimensional Modeling?","content":"Dimensional modeling (DM) is part of the Business Dimensional Lifecycle methodology developed by [[Ralph Kimball]], which includes a set of methods, techniques, and concepts for use in [Data Warehouse](term/data%20warehouse.md) design.\n\nAs a bottom-up approach, the approach focuses on identifying the critical business processes within a business and modeling and implementing these before adding additional business processes.  An alternative approach from [[Bill Inmon]] advocates a top-down design of the model of all the enterprise data using tools such as Entity-Relationship Modeling (ER).\n\nRead more on [Data Modeling with SQL and dbt](https://airbyte.com/blog/sql-data-modeling-with-dbt).\n\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/dimensions":{"title":"What are Dimensions?","content":"\nDimensions are the categorical buckets that can be used to segment, filter, or group—such as sales amount region, city, product, color, and distribution channel. Traditionally known from [OLAP](term/olap%20(online%20analytical%20processing).md) cubes with [Bus Matrixes](term/bus%20matrix.md).\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/duckdb":{"title":"What is DuckDB?","content":"\n[DuckDB](https://duckdb.org/) is an in-process SQL [OLAP](term/olap%20(online%20analytical%20processing).md) database management system. It has strong support for SQL. DuckDB is borrowing the SQLite shell implementation. Each database is a single file on disk. It's [SQLite DB](https://www.sqlite.org) for **analytical (OLAP) workloads**, whereas SQLite is for a relational database. But it can handle vast amounts of data locally. It's the smaller, lighter version of [Apache Druid](Apache%20Druid) and other OLAP technologies.\n\nIt's designed to work as an embedded library, eliminating the network latency you usually get when talking to a database.\n\u003e [!note] Skip working with error-prone Excels or CSVs directly\n\u003e\n\u003e With DuckDB, we no longer need to use plain files (CSV, Excel, Parquet). DuckDB supports schema, types, and SQL interface and is super fast. \n\n## Use-Cases\n- Ulta fast analytical use-case locally. E.g., the Taxi example includes a 10 Year, 1.5 Billion row Taxi data example that still works on a laptop. See benchmarks below. \n- It can be used as an SQL wrapper with zero copies (on top of parquets in S3). \n- Bring your **data to the users** instead of having big roundtrips and latency by doing REST calls. Instead, you can put data inside the client. You can do 60 frames per second as data is where the query is.\n\nCheck out [Rill Data](https://www.rilldata.com/), a [BI tool](term/business%20intelligence%20tools.md) that delivers sub-second interactivity because it’s backed by DuckDB (and [Druid](Apache%20Druid) for our enterprise-grade cloud services).\n\n[MotherDuck](https://motherduck.com/) is the managed service around DuckDB that lets you scale from a local DB to a cloud DB and hybrid—done by one of [Google BigQuery](Google%20BigQuery) creators or developers such as [[Jordan Tigani]]. Check his discussion on the [Analytics Engineering Podcast about The Personal Data Warehouse](https://open.spotify.com/episode/3CmeFOuIOg91xApdjbWqey?si=CmelGaxBTZ-Z-BR3fvMjmg\u0026utm_source=copy-link\u0026nd=1). The stimulating conversation around connected WebAssembly, e.g., Is an application compiled to C code, which is super fast. E.g., Figma is using that, which would otherwise never work in a browser. \n\n## Benchmarks\n- [Fast Analysis With DuckDB + Pyarrow](https://tech.gerardbentley.com/python/data/intermediate/2022/04/26/holy-duck.html) - 2022\n- [Tweet](https://mobile.twitter.com/medriscoll/status/1554698141789614081): Impressively fast, collaborative exploratory data analytics over a 20+ million row data set, hosted in the cloud with Drifting’s Jamsocket + Rill Data + DuckDB - 2022\n- [Taking DuckDB for a spin | Uwe’s Blog](https://uwekorn.com/2019/10/19/taking-duckdb-for-a-spin.html) - 2019\n\n## Example Projects\n- [Modern Data Stack in a Box with DuckDB](https://duckdb.org/2022/10/12/modern-data-stack-in-a-box.html): A fast, free, and open-source Modern Data Stack (MDS) can now be fully deployed on your laptop or to a single machine using the combination of DuckDB, Meltano, dbt, and Apache Superset. \n- [Fully Featured Project Example](https://github.com/dagster-io/dagster/blob/master/examples/project_fully_featured/) (s3, dbt, parquet, and snowflake) reading from Hackernews orchestrated with dagster.\n\n## Tech and Papers\nIt ships as an [amalgamation](https://www.sqlite.org/amalgamation.html) build - a single giant C++ file (SQLite is a single giant C file). And it's also backed up by some strong computer science. It's by the academic researchers behind MonetDB and includes implementations of a bunch of interesting papers:\n-   [Data Management for Data Science - Towards Embedded Analytics](https://www.duckdb.org/pdf/CIDR2020-raasveldt-muehleisen-duckdb.pdf) (CIDR 2020)\n-   [DuckDB: an Embeddable Analytical Database](https://www.duckdb.org/pdf/SIGMOD2019-demo-duckdb.pdf) (SIGMOD 2019 Demo)\n\nFrom [Why DuckDB](https://duckdb.org/why_duckdb):\n\u003e DuckDB contains a **columnar-vectorized query execution engine**, where queries are still interpreted, but a large batch of values (a “vector”) is processed in one operation. This dramatically reduces overhead in traditional systems such as PostgreSQL, MySQL or SQLite, which process each row sequentially. Vectorized query execution leads to far better performance in OLAP queries.\n\n\n## Python API and Handling Data\nFrom [DuckDB Docs - Python API](https://duckdb.org/docs/api/python)\n### Fetching Data with SQL\n```python\n# fetch as pandas data frame\ndf = con.execute(\"SELECT * FROM items\").fetchdf()\nprint(df)\n#        item   value  count\n# 0     jeans    20.0      1\n# 1    hammer    42.2      2\n# 2    laptop  2000.0      1\n# 3  chainsaw   500.0     10\n# 4    iphone   300.0      2\n\n# fetch as dictionary of numpy arrays\narr = con.execute(\"SELECT * FROM items\").fetchnumpy()\nprint(arr)\n# {'item': masked_array(data=['jeans', 'hammer', 'laptop', 'chainsaw', 'iphone'],\n#              mask=[False, False, False, False, False],\n#        fill_value='?',\n#             dtype=object), 'value': masked_array(data=[20.0, 42.2, 2000.0, 500.0, 300.0],\n#              mask=[False, False, False, False, False],\n#        fill_value=1e+20), 'count': masked_array(data=[1, 2, 1, 10, 2],\n#              mask=[False, False, False, False, False],\n#        fill_value=999999,\n#             dtype=int32)}\n```\n### Create Table\n```python\n# create a table\ncon.execute(\"CREATE TABLE items(item VARCHAR, value DECIMAL(10,2), count INTEGER)\")\n# insert two items into the table\ncon.execute(\"INSERT INTO items VALUES ('jeans', 20.0, 1), ('hammer', 42.2, 2)\")\n\n# retrieve the items again\ncon.execute(\"SELECT * FROM items\")\nprint(con.fetchall())\n# [('jeans', 20.0, 1), ('hammer', 42.2, 2)]\n```\n### Insert Data\n```python\n# insert a row using prepared statements\ncon.execute(\"INSERT INTO items VALUES (?, ?, ?)\", ['laptop', 2000, 1])\n\n# insert several rows using prepared statements\ncon.executemany(\"INSERT INTO items VALUES (?, ?, ?)\", [['chainsaw', 500, 10], ['iphone', 300, 2]] )\n\n# query the database using a prepared statement\ncon.execute(\"SELECT item FROM items WHERE value \u003e ?\", [400])\nprint(con.fetchall())\n# [('laptop',), ('chainsaw',)]\n```\n\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/elt":{"title":"What is ELT?","content":"Compared to [ETL](term/etl.md), in an ELT (extract load and transform) pipeline, data enrichment and transformation occur inside the [Data Warehouse](term/data%20warehouse.md), not in a processing server like in the case of ETL pipelines. The shift has been primarily made possible thanks to the appearance of cloud-based data warehouses like Redshift, BigQuery, or Snowflake.\n\nIt is also tightly connected with [Reverse ETL](term/reverse%20etl.md). And you can read more on [Reverse ETL Explained](https://airbyte.com/blog/reverse-etl#so-what-is-a-reverse-etl) or [Airbyte.com](https://airbyte.com).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/etl":{"title":"What is ETL?","content":"ETL (extract transform and load) is loading data in a three-phase process and is a classic paradigm that emerged in the 1970s. Recently, it has shifted to [ELT](term/elt.md). The ELT philosophy dictates that data should be untouched – apart from minor cleaning and filtering – as it moves through the extraction and loading stages so that the raw data is always accessible in the [Data Warehouse](term/data%20warehouse.md).\n\n## ETL is Changing\nThe way we do ETL is changing. For a long time it was done with ETL tools such as Informatica, IBM Datastage, Cognos, AbInitio, or Microsoft SSIS, today we use more programmatic or configuration-driven platforms like [[Airflow]], [[Dagster]], and [Temporal](term/temporal.md). And with data needs growing, and the need for having data faster, the trend shifted to ELT.\n\nHistorically **ETL was once preferred** over ELT for the following **no-longer-valid reasons**: \n- ETL could achieve cost savings by removing unwanted data before sending it to the destination –  however, with the plummeting cost of cloud-based computation and storage the value of this proposition is greatly reduced. \n- Because ETL transforms data before it is stored, it avoids the complexity of transforming data _after_ sending it to the destination – however, new tools such as [[dbt]] (data build tool) make it preferable and easy to transform data in the destination.\n\nSee also [ETL vs ELT](term/etl%20vs%20elt.md).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/etl-elt-airbyte":{"title":"ETL and ELT with Airbyte","content":"[ELT](term/elt.md) and [ETL](term/etl.md) specific to Airbyte mean:\n- **Extract**: Retrieve data from a [source](https://docs.airbyte.com/integrations/#Sources), which can be an application, database, or anything really.\n- **Load**: Move data to your [destination](https://docs.airbyte.com/integrations/#Destinations).\n- **Transform**: Clean up the data. This is referred to as [normalization](term/normalization.md) in Airbyte and involves [incremental synchronization](term/incremental%20synchronization.md) and [deduplication](https://docs.airbyte.com/understanding-airbyte/connections/incremental-deduped-history), changing data types, formats, and more.","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/etl-vs-elt":{"title":"ETL vs. ELT","content":"[ETL](term/etl.md) (Extract Transform and Load) and [ELT](term/elt.md) (Extract Load and Transform). ETL was originally used for [Data Warehousing](term/data%20warehouse.md) and ELT for creating a [Data Lake](term/data%20lake.md). The key difference is that the data schema and the transformation need to be done before the data lands at the destination.\n\n## Disadvantages of ETL compared to ELT\n\n**ETL** has several **disadvantages compared to ELT**, including the following:\n\n- Generally, only transformed data is stored in the destination system, and so analysts must know beforehand every way they are going to use the data, and every report they are going to produce.  \n- Modifications to requirements can be costly, and often require re-ingesting data from source systems.\n- Every transformation that is performed on the data may obscure some of the underlying information, and analysts only see what was kept during the transformation phase. \n- Building an ETL-based data pipeline is often beyond the technical capabilities of analysts.\n\nFind more on [An overview of Airbyte’s replication modes](https://airbyte.com/blog/understanding-data-replication-modes).\n\n## ELT/ETL Tool Comparision\nNeed to find the best data integration tool for your business? Which platform integrates with hour data sources and destinations? Which one provides the features you’re looking for?  \n  \nWe made it simple for you. Here’s a [spreadsheet](https://docs.google.com/spreadsheets/d/1QKrtBpg6PliPMpcndpmkZpDVIz_o6_Y-LWTTvQ6CfHA/edit?usp=sharing) with a comparison of all those actors. Or an extensive detailed comparison between the tools on [Top ETL tools compared in details](https://airbyte.com/etl-tools-comparison).\n\nSee also more on [Airbyte.com](https://airbyte.com) or [Reverse ETL Explained](https://airbyte.com/blog/reverse-etl#so-what-is-a-reverse-etl).\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/etlt":{"title":"What is EtLT?","content":"[EtLT](https://airbyte.com/blog/etlt-gdpr-compliance) refers to Extract, “tweak”, Load, Transform. This is an approach to data integration – in which a very light “tweak” (small “t”) transformation is done on the data before it reaches the destination, as demonstrated in the following image:\n\n![](images/etlt-extract-tweak-load-transform.png)\n\nSee more on [EtLT for improved GDPR compliance](https://airbyte.com/blog/etlt-gdpr-compliance).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/full-refresh-synchronization":{"title":"What is Full Refresh Synchronization?","content":"A Full Refresh Sync will attempt to retrieve all data from the source every time a sync is run. Then there are two choices, *Overwrite* and *Append*. Overwrite deletes the data in the destination before running the sync and Append doesn't.\n\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/functional-data-engineering":{"title":"What is Functional Data Engineering?","content":"Functional Data Engineering brings _clarity_. When functions are \"pure,\" they do not have side effects. They can be written, tested, reasoned about, and debugged in isolation without understanding the external context or history of events surrounding their execution. Its [Functional Programming](term/functional%20programming.md) applied to the field of data engineering initiated by [Maxime Beauchemin](term/maxime%20beauchemin.md) with [Functional Data Engineering — a modern paradigm for batch data processing](https://maximebeauchemin.medium.com/functional-data-engineering-a-modern-paradigm-for-batch-data-processing-2327ec32c42a).\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/functional-programming":{"title":"What is Functional Programming?","content":"\nFunctional Programming is a style of building functions that threaten computation as a mathematical function that avoids changing state and mutable data. It is a declarative programming paradigm, which means programming expressive and [declarative](term/declarative.md) as opposed to imperative. It's getting more popular with the rise of [Functional Data Engineering](term/functional%20data%20engineering.md).\n\nSee also [Programming Languages](term/programming%20languages.md).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/general-infos":{"title":"General Infos (Folder Structure, Links)","content":"[Quartz](https://quartz.jzhao.xyz) runs on top of [Hugo](https://gohugo.io/) so all notes are written in [Markdown](https://www.markdownguide.org/getting-started/) and can be edited through any editor.\n\n## Folder Structure\nThe content of the Glossary is in `content/term` folder. The rest outside of `content/` is the website/framework.\n\nTo edit the main home page, open `/content/_index.md`.\n## Links\nTo create a link between terms in the glossary, just create a normal link using Markdown pointing to the document in question. Please note that **all links should be relative to the root `/content` path**.\n```markdown\nFor example, I want to link this current document to `term/config.md`.\n[A link to the config page](term/config.md)\n```\n\nSimilarly, you can put local images anywhere in the `/content` folder.\n```markdown\nExample image (source is in content/images/example.png)\n![Example Image](/content/images/example.png)\n```\n\n## Lower Case\nTerms are lower case that links are also lowercase. When we create a link to a term, I usually capitalize the beginning of each word to make it look nice. E.g `[Apache Arrow](term/apache%20arrow.md)`. Other such as YAML I write all in capitals.\n\nWe didn't activate wikilinks, but that would be an option as well. See more on [editing](https://quartz.jzhao.xyz/notes/editing/).\n## Metatag with Front Matter\nHugo is picky when it comes to metadata for files. Make sure that your title is double-quoted and that you have a title defined at the top of your file like so. You can also add tags here as well.\n```yaml\n---\ntitle: \"What is a Glossary?\"\ntags:\n- example-tag\n- here i can add more we keep it lower case\nurl: \"term/my-other-domain\"\naliases:\n- Digital Garden\n- Second Brain\n---\n\nRest of your content here.\n```\n\n- `url`: this is not needed, only if the default link (name of the note) is not sufficient\n\t- all spaces will be replaced with `-` (dash).\n- `aliases`: Are like tags, you can add multiple and they will be linkable same as a adding a new term would be.\n\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/granularity":{"title":"What is Granularity","content":"Declaring the granularity (or _grain_) is the pivotal step in [Dimensional Modeling](term/dimensional%20modeling.md). The grain establishes exactly what a single fact table row represents. The grain declaration becomes a binding contract on the design. The grain must be declared before choosing [[dimensions]] or [[facts]] because every candidate dimension or fact must be consistent with the grain. This consistency enforces uniformity on all dimensional designs which is critical to [Business Intelligence](term/business%20intelligence.md) application performance and ease of use.\n\nFor example, in the **transformation layer**, you must balance low and high granularity. What level do you aggregate and store (e.g., [rollups](term/rollup.md) hourly data to daily to save storage), or what valuable dimensions to add. With each dimension and its column added, rows will [explode](https://www.ibm.com/docs/en/ida/9.1.1?topic=phase-step-identify-measures#c_dm_design_cycle_4__c_dm_4_step7) exponentially, and we can’t persist each of these representations to the filesystem.\n\nA [Semantic Layer](term/semantic%20layer.md) is much more flexible and makes the most sense on top of transformed data in a [Data Warehouse](term/data%20warehouse.md). Avoid extensive reshuffles or reprocesses of large amounts of data. Think of [OLAP](term/olap%20(online%20analytical%20processing).md) cubes where you can dice-and-slice ad-hoc on significant amounts of data without storing them ahead of time\n\nRead more on [Kimball Dimensional Modeling Techniques](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/grain/). Also related is [Rollup](term/rollup.md).\n\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/idempotency":{"title":"What is Idempotency?","content":"Idempotency is the property of a particular operation that can be applied multiple times without changing the resulting outcome by being given the same inputs. It is used in [Functional Programming](term/functional%20programming.md) and was the foundation for [Functional Data Engineering](term/functional%20data%20engineering.md).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/imperative":{"title":"What is imperative?","content":"\nAn **imperative** pipeline tells _how_ to proceed at each step in a procedural manner. In contrast, a **[declarative](term/declarative.md)** data pipeline does not tell the order it needs to be executed but instead allows each step/task to find the best time and way to run. \n\nThe *how* should be taken care of by the tool, framework, or platform running on. For example, update an asset when upstream data has changed. Both approaches result in the same output. However, the declarative approach benefits from **leveraging compile-time query planners** and **considering runtime statistics** to choose the best way to compute and find patterns to reduce the amount of transformed data.\n\nRead more on [Data Orchestration Trends: The Shift From Data Pipelines to Data Products](https://airbyte.com/blog/data-orchestration-trends).\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/in-memory-format":{"title":"What is an In-Memory Format?","content":"\nIn-memory formats are optimized to:\n- hit fast instruction sets \n- be cache friendly \n- be parallelizable\n\nFormats:\n- [Apache Arrow](term/apache%20arrow.md) \n- [Apache Spark](Apache%20Spark) [[DataFrames]]\n- [NumPy](term/numpy.md)\n- [Pandas](term/pandas.md)\n\nThe opposed to in-memory formats are [Data Lake File Formats](Data%20Lake%20File%20Formats) which save space, be cross-language and serve as long-term storage. More on the Data AI Summit talk about [From bits to Data Frames](https://microsites.databricks.com/sites/default/files/2022-07/Sound-Data-Engineering-in-Rust_From-Bits%20to-DataFrames.pdf) by Jorge C Leitao.\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/incremental-synchronization":{"title":"What is Incremental Synchronization?","content":"Incremental synchronization is a process which efficiently copies data to a destination system by periodically executing queries on a source system for records that have been updated or inserted since the previous sync operation. Only those records that have been recently inserted or updated will be sent to the destination, which is much more efficient than copying an entire data set on each sync operation. Incremental synchronization makes use of a cursor field such as `updated_at` (or whatever you wish to call the field) to determine which records should be propagated, and only records with an `updated_at` value that is newer than the `updated_at` value of the most recent record sent in the previous sync should be replicated.\n\nHowever, without special consideration, records that have been deleted in the source system will not be propagated to the destination as they will never appear in the results from such a query. This may be addressed by [Soft Deletes](term/soft%20delete.md) or by making use of [CDC replication](https://airbyte.com/blog/change-data-capture-definition-methods-and-benefits).\n\nRead more on [Incremental data synchronization between Postgres databases](https://airbyte.com/tutorials/incremental-data-synchronization) or see related [Full Refresh Synchronization](term/full%20refresh%20synchronization.md).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/jinja-template":{"title":"What is a Jinja Template?","content":"\nJinja is a fast, expressive, extensible templating engine. Special placeholders in the template allow writing code similar to [Python](term/python.md) syntax. Then the template is passed data to render the final document.\n\nMost popularized by [[dbt]].  Read more on the [Jinja Documentation)](https://jinja.palletsprojects.com/).\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/key-performance-indicator-kpi":{"title":"What is Key Performance Indicator (KPI)?","content":"A performance indicator or key performance indicator (KPI) is a type of performance measurement. KPIs evaluate the success of an organization or of a particular activity (such as projects, programs, products, and other initiatives) in which it engages. \n\nSee more on [What is a Metric](term/metric.md), as it's a synonym with a KPI.\n\n\n\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/kubernetes":{"title":"What is Kubernetes?","content":"It’s a platform that allows you to run and orchestrate container workloads. [**Kubernetes**](https://stackoverflow.blog/2020/05/29/why-kubernetes-getting-so-popular/) **has become the de-facto standard** for your cloud-native apps to (auto-) [scale-out](https://stackoverflow.com/a/11715598/5246670) and deploy your open-source zoo fast, cloud-provider-independent. No lock-in here. Kubernetes is the **move from infrastructure as code** towards **infrastructure as data**, specifically as [YAML](term/yaml.md). With Kubernetes, developers can quickly write applications that run across multiple operating environments. Costs can be reduced by scaling down.","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/lambda-architecture":{"title":"What is a Lambda Architecture?","content":"Lambda architecture is a data-processing architecture designed to handle massive quantities of data by taking advantage of both batch and stream-processing methods. This approach to architecture attempts to balance latency, throughput, and fault tolerance using batch processing to provide comprehensive and accurate views of batch data, while simultaneously using real-time stream processing to provide views of online data. The two view outputs may be joined before the presentation. The rise of lambda architecture is correlated with the growth of big data, real-time analytics, and the drive to mitigate the latencies of [MapReduce](term/map%20reduce.md).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/machine-learning":{"title":"What is Machine Learning?","content":"Machine learning (ML) is a type of artificial intelligence (AI) that allows software applications to become more accurate at predicting outcomes without being explicitly programmed to do so. Machine learning [algorithms](https://www.techtarget.com/whatis/definition/algorithm) use historical data as input to predict new output values.\n\nMore on [Machine Learning  | Tech Target](https://www.techtarget.com/searchenterpriseai/definition/machine-learning-ML).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/map-reduce":{"title":"What is MapReduce?","content":"MapReduce is a programming paradigm that enables massive scalability across hundreds or thousands of servers in a Hadoop cluster. As the processing component, MapReduce is the heart of [Apache Hadoop](term/apache%20hadoop.md). The term \"MapReduce\" refers to two separate and distinct tasks that Apache Hadoop programs perform.","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/master-data-management-mdm":{"title":"What is Master Data Management (MDM)?","content":"Master data management is a method to centralize master data. It's the bridge between the business that maintain the data and know them best and the data folks, and it's a tool of choice. It helps with uniformity, accuracy, stewardship, semantic consistency, and accountability of mostly enterprise master [Data Assets](term/data%20asset.md).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/maxime-beauchemin":{"title":"Maxime Beauchemin","content":"Creator of [[Apache Airflow]] and [Apache Superset](term/apache%20superset.md).\n\nStarted as [Business Intelligence](term/business%20intelligence.md) Engineer and is working now at [[Preset]] (Superset as a Service).\n\nStarter of [idempotency](term/idempotency.md) and [functional data engineering](term/functional%20data%20engineering.md).\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/metric":{"title":"What is a Metric?","content":"\nA Metric, also called [KPI](term/key%20performance%20indicator%20(kpi).md) or (calculated) measure, are terms that serve as the building blocks for how business performance is both measured and defined, as knowledge of how to define an organization's KPIs. It is fundamental to have a common understanding of them. Metrics usually surface as business reports and dashboards with direct access to the entire organization.\n\nFor example, think of operational metrics that represent your company's performance and service level or financial metrics that describe its financial health. Today these metrics are primarily defined in a lengthy [SQL](term/sql.md) statement inside the [BI tools](term/business%20intelligence%20tools.md). \n\nCalculated measures are part of metrics and apply to specific [dimensions](term/dimensions.md) traditionally mapped inside a [Bus Matrix](term/bus%20matrix.md). Dimensions are the categorical buckets that can be used to segment, filter, group, slice, and dice—such as sales amount, region, city, product, color, and distribution channel. Dimensions (and facts) are also known from the concept of [Dimensional Modeling](term/dimensional%20modeling.md). \n\nSee more on [Semantic Layer](term/semantic%20layer.md) or [Metrics Layer](term/metrics%20layer.md).\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/metrics-layer":{"title":"What is a Metrics Layer?","content":"\u003e [!info] Similarities to a Semantic Layer\n\u003e \n\u003e The metrics layer is one component of a [Semantic Layer](term/semantic%20layer.md). A limited metrics layer is usually built into a [BI tool](term/business%20intelligence%20tools.md), translating its [metrics](term/metric.md) to only that BI tool. \n\nA metrics layer sometimes also called metrics store, includes a specification of metrics such as [Measures](term/metric.md) and [dimensions](term/dimensions.md). Additionally, it can contain model parsing from files (mostly [YAML](term/yaml.md) and APIs to create and execute metric logic; some include a cache layer. A Metrics Layers encourages us to enforce the [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself) (Do not repeat yourself) principle by defining it once and population it to any BI tools used or integrated into internal applications or processes.\n\nRead more on [Semantic Layer](term/semantic%20layer.md) or [The Rise of the Semantic Layer](https://airbyte.com/blog/the-rise-of-the-semantic-layer-metrics-on-the-fly).\n\n\n‍","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/modern-data-stack":{"title":"What is the Modern Data Stack?","content":"The Modern Data Stack (MDS) is a heap of open-source tools to achieve end-to-end analytics from ingestion to transformation to ML over to a columnar data warehouse or lake solution with an analytics BI dashboard backend. This stack is extendable like lego blocks. Usually, it consists of **data integration, a transformation tool, an [Orchestrator](term/data%20orchestrator.md), and a [Business Intelligence Tool](term/business%20intelligence%20tools.md)**. With growing data, you might add [Data Quality](term/data%20quality.md) and observability tools, [Data Catalog](term/data%20catalog.md), [Semantic Layer](term/semantic%20layer.md), and more.\n\nIn a way, it is [unbundling](https://blog.fal.ai/the-unbundling-of-airflow-2/) the data stack as Gorkem says:\n\u003e Products start small, in time, add adjacent verticals and functionality to their offerings, and become a platform. Once these **platforms** become big enough, people begin to figure out how to serve better-neglected verticals or abstract out functionality to break it down into purpose-built chunks, and the unbundling starts.\n\nThe goal of an MDS is to get data insight with the best suitable tools for each part. It's noteworthy that it's a relatively new term.\n\n\u003e [!note] New Terms popping up\n\u003e\n\u003e There is already a new term [ngods (new generation open-source data stack)](https://blog.devgenius.io/modern-data-stack-demo-5d75dcdfba50). Or *DataStack 2.0* in Dagster's recent [blog post](https://dagster.io/blog/evolution-iq-case-study).\n\n## The Future of MDS\nIf we look a little in the future, Barr Moses illustrates in her article [What's In Store For The Future Of The Modern Data Stack?](https://www.montecarlodata.com/blog-the-future-of-the-modern-data-stack/) more features such as data sharing, universal [Data Governance](term/data%20governance.md), [Data Lake](term/data%20lake.md), and [Data Warehouse](term/data%20warehouse.md) equalized, and a newer evolution of predictive analysis:\n![](images/future-modern-data-stack.png)","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/normalization":{"title":"Airbyte Normalization","content":"Normalization is the process of structuring data from the source into a format appropriate for consumption in the destination. For example, when writing data from a nested, dynamically typed source like a JSON API to a relational destination like Postgres, normalization is the process that un-nests JSON from the source into a relational table format that uses the appropriate column types in the destination.\n\n\nRead more on our [docs](https://docs.airbyte.com/cloud/core-concepts#normalization).\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/notebooks":{"title":"What are Notebooks?","content":"\nData notebooks popularized by Jupyter notebooks are the centralized IDE inside a browser for doing collaborative work.\n\n1. Notebooks that are popularized and in heavy use today.\n\t- [Jupyter Notebook](https://jupyter.org/) and [JupyterHub](https://jupyter.org/hub)\n\t\t- Automation on top of Jupyter notebooks: [Naas](https://github.com/jupyter-naas/awesome-notebooks)\n\t- [Zeppelin](https://zeppelin.apache.org/)\n\t- [Databricks Notebook](https://docs.databricks.com/notebooks/index.html)\n2. Newer versions of Jupyter notebooks with more integrated features and an integrated cloud\n\t- [HEX](https://hex.tech/)\n\t- [Deepnote](https://deepnote.com/)\n\t- [Count.co](https://count.co)\n\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/olap-online-analytical-processing":{"title":"What is OLAP (Online Analytical Processing)?","content":"\nOLAP is an acronym for **Online Analytical Processing**. OLAP performs multidimensional analysis of business data and provides the capability for complex calculations, trend analysis, and sophisticated [data modeling](term/data%20modeling.md). An **OLAP cube** is a multidimensional database that is optimized for [data warehouse](term/data%20warehouse.md) and online analytical processing (OLAP) applications. \n\nAn OLAP cube is a method of storing data in a multidimensional form, generally for reporting purposes. In OLAP cubes, data ([Measures](term/metric.md)) are categorized by [dimensions](term/dimensions.md). \n\nIn order to manage and perform processes with an OLAP cube, Microsoft developed a query language, known as [multidimensional expressions (MDX)](https://learn.microsoft.com/en-us/analysis-services/multidimensional-models/mdx/), in the late 1990s.  Many other vendors of multidimensional databases have adopted MDX for querying data, but with this specific language, management of the cube requires personnel with the skill set.\n\nThe opposite of OLAP is [OLTP](term/oltp%20(online%20transactional%20processing).md). Read more on [Wikipedia](https://en.wikipedia.org/wiki/Online_analytical_processing).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/oltp-online-transactional-processing":{"title":"What is OLTP (Online Transactional Processing)=","content":"In online transaction processing (**OLTP**), information systems typically facilitate and manage **transaction-oriented** applications. It's the opposite of [OLAP (Online Analytical Processing)](term/olap%20(online%20analytical%20processing).md).\n\nRead more on [Wikipedia](https://en.wikipedia.org/wiki/Online_transaction_processing).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/orc":{"title":"What is ORC?","content":"The **Optimized Row Columnar** (ORC) [Data Lake File Format](term/data%20lake%20file%20format.md) provides a highly efficient way to store Hive data. It was designed to overcome the limitations of the other Hive file formats. Using ORC files improves performance when Hive is reading, writing, and processing data.\n\nRead more about how to build a Data Lake on top of it on our [Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/other-glossaries":{"title":"Other Data Glossaries","content":"Other helpful data glossaries:\n- [Seconda Glossary](https://www.secoda.co/glossary) \n- [Prisma's Data Guide](https://www.prisma.io/dataguide/)\n- [Reddit Data Engineering Wiki](https://dataengineering.wiki/)","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/pandas":{"title":"What is Pandas?","content":"Pandas is a software library written for the [Python](term/python.md) programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series. It is free software released under the three-clause BSD license. \n\n## What is a Pandas DataFrame\nTwo-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nThe data structure also contains labeled axes (rows and columns). Arithmetic operations align on both row and column labels. It can be thought of as a dict-like container for Series objects. The primary Pandas data structure.\n\nSee more on [Pandas Documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html).\n\nAnother DataFrame with the same API is [Koalas](https://github.com/databricks/koalas), created by Databricks, optimized for more extensive data sets, and [Apache Spark](term/apache%20spark.md).\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/partial-success":{"title":"Partial Success","content":"A Partial Success indicates that some records were successfully committed to the destination during a sync, even when the overall sync status was reported as a failure.\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/programming-languages":{"title":"Programming Languages?","content":"2022 marks JavaScript’s tenth year in a row as the most commonly used programming language according to [Stack Overflow Developer Survey 2022](https://survey.stackoverflow.co/2022/#section-most-popular-technologies-programming-scripting-and-markup-languages). Further, they say: People learning to code are more likely than Professional Developers to report using Python (58% vs 44%), C++ (35% vs 20%), and C (32% vs 17%).\n\nProgramming Languages (so far):\n- [SQL](term/sql.md)\n- [Python](term/python.md)\n\nSee also [Functional Programming](term/functional%20programming.md) or [Functional Data Engineering](term/functional%20data%20engineering.md).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/push-down":{"title":"What is a Push-Down?","content":"Query pushdown aims to execute as much work as possible in the source databases. \n\nPush-downs or query pushdowns push transformation logic to the source database. This reduces to store data physically and transfers them over the network. \n\nFor example, a [semantic layer](term/semantic%20layer.md) or [data virtualization](term/data%20virtualization.md) translates the transformation logic into [SQL](term/sql.md) queries and sends the SQL queries to the database. The source database runs the SQL queries to process the transformations.\n\nPushdown optimization increases mapping performance when the source database can process transformation logic faster than the semantic layer itself. \n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/python":{"title":"What is Python?","content":"Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically-typed and garbage-collected. It supports multiple programming paradigms, including structured, object-oriented, and [Functional Programming](term/functional%20programming.md).\n\nPython is the de facto standard for [Data Engineering](term/data%20engineering.md) next to [SQL](term/sql.md). If you want to learn Python, see the Freecodecamp Python Course in under 300 hours:\n{{\u003c youtube vMl4YUch7x4 \u003e}}\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/raw-tables":{"title":"Raw Tables","content":"Airbyte spits out tables with the prefix `_airbyte_raw_`. This is your replicated data, but the prefix indicates that it's not normalized. If you select basic [normalization](term/normalization.md), Airbyte will create renamed versions without the prefix.","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/reverse-etl":{"title":"What is Reverse ETL?","content":"Reverse ETL is the flip side of the [ETL](term/etl.md)/[ELT](term/elt.md). **With Reverse ETL, the data warehouse becomes the source rather than the destination**. Data is taken from the warehouse, transformed to match the destination's data formatting requirements, and loaded into an application – for example, a CRM like Salesforce – to enable action.\n\nIn a way, the Reverse ETL concept is not new to data engineers, who have been enabling data movement warehouses to business applications for a long time. As [Maxime Beauchemin](term/maxime%20beauchemin.md) mentions in [his article](https://preset.io/blog/reshaping-data-engineering/), Reverse ETL “appears to be a modern new means of addressing a subset of what was formerly known as  [Master Data Management (MDM)](term/master%20data%20management%20(mdm).md).”\n\nRead more about in [Reverse ETL Explained](https://airbyte.com/blog/reverse-etl#so-what-is-a-reverse-etl).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/rollup":{"title":"What is a Rollup?","content":"Rollup is a form of summarization or pre-aggregation. Rolling up data can dramatically reduce the size of data to be stored and reduce row counts by potential orders of magnitude.\n\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/rust":{"title":"What is Rust?","content":"Former Mozilla employee Graydon Hoare initially created [Rust](https://glossary.airbyte.com/term/rust) as a personal project. The first stable release, Rust 1.0 was released on May 15, 2015. Rust is a [**multi-paradigm programming language**](https://en.wikipedia.org/wiki/Comparison_of_multi-paradigm_programming_languages) that supports imperative procedural, concurrent actor, object-oriented and pure [functional](https://glossary.airbyte.com/term/functional-programming) styles, supporting generic programming and metaprogramming statically and dynamically.\n\n\u003e The goal of Rust is to be a good programming language for creating highly **concurrent, safe, and performant systems**. [Learning Rust](https://learning-rust.github.io/docs/a1.why_rust.html)\n\nFind more comparisons to [python](term/python.md) and how Rust will take over [data engineering](term/data%20engineering.md) on [Will Rust Take over Data Engineering?](https://airbyte.com/blog/rust-for-data-engineering).\n\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/schema-evolution":{"title":"What is Schema Evolution?","content":"Automatic Schema Evolution is a crucial feature in [Data Lake Table Format](term/data%20lake%20table%20format.md)s as changing formats is still a pain in today's data engineer work. Schema Evolution means adding new columns without breaking anything or even enlarging some types. You can even rename or reorder columns, although that might break backward compatibilities. Still, we can change one table, and the table format takes care of switching it on all distributed files. Best of all does not require e rewrite of your table and underlying files.\n\nSee also [ACID Transactions](term/acid%20transactions.md).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/semantic-layer":{"title":"What is a Semantic Layer","content":"\n\u003e A semantic layer (sometimes also called Headless BI) calculates complex business [metrics](term/metric.md) at *query time*. It sits between your data sources/transformation layer and your analytics tools. You define a metric's aggregations (daily, weekly, monthly, and quarterly) and dimensions (region, customer, product). Examples of metrics could be \"monthly active users\", \"weekly revenue\", \"number of paying customers\", and so on.\n\nYou can think of a semantic layer as a translation layer between any data presentation layer ([business intelligence](term/business%20intelligence.md), [notebooks](term/notebooks.md), data apps) and the data sources. A translation layer includes many features, such as integrating data sources, modeling the metrics, and integrating with the data consumers by translating metrics into [SQL](term/sql.md), REST, or GraphQL.\n\nBecause everyone has different definitions of “active” users or “paying” customers, the semantic layer allows you to define these discrepancies once company-wide. Instead of having three different versions each presentation tool e.g. BI tool would show a different number than your Jupyter notebook or data app. And what if the metric changes to a new definition, with a semantic layer you change only one time. This powerful feature empowers domain experts and data practitioners to get a common understanding of business metrics.\n\nA sub-layer of the semantic layer is the [Metrics Layer](term/metrics%20layer.md). \n\nRead more on [The Rise of the Semantic Layer](https://airbyte.com/blog/the-rise-of-the-semantic-layer-metrics-on-the-fly) or other fascinating reads on the topic:\n-   [Down the Semantic Rabbit Hole](https://jpmonteiro.substack.com/p/down-the-semantic-rabbit-hole)\n-   [The Missing Piece of the Modern Data Stack](https://benn.substack.com/p/metrics-layer) \n-   [Deep Dive: What The Heck Is the Metrics Layer](https://pedram.substack.com/p/what-is-the-metrics-layer)\n-   [The Great Data Debate by Atlan](https://atlan.com/great-data-debate/)\n-   [The Metrics Layer has Growing up to do](https://prakasha.substack.com/p/the-metrics-layer-has-growing-up)\n-   [The Universal Semantic Layer, More Important Than Ever](https://www.atscale.com/blog/what-is-a-universal-semantic-layer-why-would-you-want-one/)\n-   [Demystifying the Metrics Store and Semantic Layer](https://thenewstack.io/demystifying-the-metrics-store-and-semantic-layer/)","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/semantic-warehouse":{"title":"What is a Semantic Warehouse","content":"It incorporates best practices espoused by [Bill Inmon](Bill%20Inmon) for robust, scalable warehouse design built for the cloud as an abstraction of the [[Modern Data Stack]] with [[term/ata modeling]] at its core. \n\n![](images/semantic-warehouse.png)\nIllustrating the Semantic Warehouse from [Chad Sanderson on LinkedIn](https://www.linkedin.com/posts/chad-sanderson_im-very-happy-to-unveil-the-semantic-warehouse-activity-6958091220157964288-JSXj/)\n\nChad Sanders first introduced the term in this [LinkedIn post](https://www.linkedin.com/posts/chad-sanderson_im-very-happy-to-unveil-the-semantic-warehouse-activity-6958091220157964288-JSXj/). Some defining features:\n- Data as a product and capturing the natural world through events instead of batch processing with a clear defined schema\n- [Data Contract](term/data%20contract.md) as a foundation to introduce contracts to its underlying source tables.\n- Collaborative, peer-reviewed data modeling.\n- Centralized metrics with a [Metrics/Locical Layer](term/metrics%20layer.md) allow collaborative data modeling between the business and the (data) engineers and abstract away the complexity of the data stack.\n- Built-in incentives with semantics and modeling are required to generate good [Data Products](term/data%20product.md).\n\nThe semantic warehouse tries to solve the following problems:\n1. The [Modern Data Stack](term/modern%20data%20stack.md) (MDS) is a good set of tools for building things, but they do not help ensure that what is being built is high quality.  \n2. Most data architectures and data foundations are not scalable. The first version of data infrastructure (typically set up by engineers or junior data devs) is never refactored because it is tough to do so  \n3. Producers do not (but should) own data quality. Data Engineers should not be middle-men caught in the cross-fire of consumers  \n4. Semantics and context are missing. Data devs spend days to weeks just trying to understand what data we have, what it means, how it maps to services, and whether data can be trusted  \n5. Data modeling was not a first-class citizen. Modeling was challenging to do (because of #4) and, in some cases, impossible, thanks to data simply being missing.  \n6. Our [Data Warehouse](term/data%20warehouse.md) did not reflect the real world. Instead, it was a dumping ground for production services and 3rd party APIs.  \n7. A lack of interoperability due to tools not 'speaking the same language.' We have multiple products which each require their distinct modeling environment and no shared understanding of business concepts  \n8. [Data Governance](term/data%20governance.md) is critical, but businesses will reject it if it becomes a bottleneck. We cannot scale our data team horizontally with the complexity  \n  \nSee also [[Metrics Layer|Semantic Layer]] and [[Data Contract]].","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/slowly-changing-dimension-scd":{"title":"What is Slowly Changing Dimension?","content":"A Slowly Changing Dimension (SCD) is **a dimension that stores and manages both current and historical data over time in a [Data Warehouse](term/data%20warehouse.md)**. It is considered and implemented as one of the most critical ETL tasks in tracking the history of dimension records.","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/soft-delete":{"title":"What is a Soft Delete?","content":"In order to propagate records that have been deleted when using [Incremental Synchronization](term/incremental%20synchronization.md) modes, records in a database may include a field that indicates that a record should be treated as if it has been removed. This is necessary because incremental synchronization does not replicate documents that are fully deleted from a source system.\n\nFor example, a boolean flag such as `is_deleted` could be used to indicate that a record should be treated as if it has been deleted. All queries would need to be written so as to exclude records/documents where `is_deleted` is set, and periodically executed background jobs can be used to remove all documents where `is_deleted` is set.\n\n‍\n","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/sql":{"title":"What is SQL?","content":"SQL is **a standardized language used to interact with relational [[databases]]**. It stands for structured query language (SQL) and defines a standard [programming language](term/programming%20languages.md) utilized to extract, organize, manage, and manipulate data stored in relational databases.\n\nHere are different levels you can go into ([Source](https://twitter.com/largedatabank/status/1559651463919452161)):\n![](images/sql-levels-explained.png)\n\nSee more on [SQL-Levels Explained](https://github.com/airbytehq/SQL-Levels-Explained).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/storage-layer-object-store":{"title":"What is a Storage Layer / Object Store?","content":"A storage layer or object storage are services from the three big cloud providers, AWS S3, Azure Blob Storage, and Google Cloud Storage. The web user interface is easy to use. **Its features are very basic, where, in fact, these object stores store distributed files exceptionally well.** They are also highly configurable, with solid security and reliability built-in.\n\nYou can build on with  [Data Lake File Format](term/data%20lake%20file%20format.md) or [Data Lake Table Format](term/data%20lake%20table%20format.md). Read more on our [Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/temporal":{"title":"Temporal","content":"\u003e [!info] Info\n\u003e \n\u003e This is only relevant for individuals who want to learn about or contribute to our underlying platform.\n\n[Temporal](https://temporal.io/) is a development kit that lets you create workflows, parallelize them, and handle failures/retries gracefully. We use it to reliably schedule each step of the ELT process, and a Temporal service is always deployed with each Airbyte installation.\n\nRead more on [How we Scale Workflow Orchestration with Temporal](https://airbyte.com/blog/scale-workflow-orchestration-with-temporal) at Airbyte.","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/time-travel":{"title":"What is Time Travel?","content":"With time travel, the [Data Lake Table Format](term/data%20lake%20table%20format.md) versions the big data you store in your [Data Lake](term/data%20lake.md). You can access any historical version of that data, simplifying data management with easy-to-audit, rollback data in case of accidental bad writes or deletes, and reproduce experiments and reports. Time travel enables reproducible queries as you can query two different versions simultaneously.\n\nRead more about how to build a Data Lake on top of it on our [Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null},"/term/yaml":{"title":"What is YAML?","content":"YAML is a data serialization language often used to write configuration files. Depending on whom you ask, YAML stands for yet another markup language, or YAML isn’t markup language (a recursive acronym), which emphasizes that YAML is for data, not documents.","lastmodified":"2022-10-21T08:35:58.319161918Z","tags":null}}