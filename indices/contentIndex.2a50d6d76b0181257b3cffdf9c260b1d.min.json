{"/":{"title":"Data Glossary 🧠","content":"\n## A Single Place for All Data Knowledge\nFind all Glossary Terms below:\n* [By Tags](tags)\n\nIf you just want to get inspired, you can start with one of these terms:\n- [Data Engineering](term/data%20engineering.md) // [Business Intelligence](term/business%20intelligence.md) // [Analytics](term/analytics.md) // [Machine Learning](term/machine%20learning.md)\n- [Data Engineering Concepts](term/data%20engineering%20concepts.md) // [Data Engineering Guides](term/data%20engineering%20guides.md) \n- [Data Lake](term/data%20lake.md) // [Data Lake Table Format](term/data%20lake%20table%20format.md) // [Data Lakehouse](term/data%20lakehouse.md)\n- [Programming Languages](term/programming%20languages.md) // [Functional Programming](term/functional%20programming.md)  // [Functional Data Engineering](term/functional%20data%20engineering.md)\n- [Airbyte Related Terms](tags/airbyte/)\n\nOtherwise, you can simply [Search](https://glossary.airbyte.com/#navigation) (`cmd/ctrl+k`) for terms and content. Or find out more [About this Glossary](term/about%20this%20glossary.md).\n\n\u003cbr\u003e\n\n\u003e [!info] How to Contribute?\n\u003e \n\u003e 1.  ⭐ Star our [GitHub](https://github.com/airbytehq/glossary) repo\n\u003e 2.  🗣️  [Share the Glossary](https://twitter.com/intent/tweet?text=Have%20you%20seen%20the%20latest%20on%20the%20%22Airbyte%20Glossary%20%F0%9F%A7%A0?%20glossary.airbyte.com)\n\u003e 3.  ✍️ Missing a Term or want to fix a typo? [Contribute to Glossary](term/contribute%20to%20glossary.md) \n\u003e 4. 👀 Want to discuss or need help, talk to us on [Slack](https://slack.airbyte.com)\n\n\n","lastmodified":"2022-09-09T09:31:21.26893918Z","tags":null},"/term/about-this-glossary":{"title":"About this Glossary","content":"The Airbyte Glossary is built on top of the [Digital Garden](https://jzhao.xyz/posts/networked-thought/) analogy. Instead of aligning all glossary terms in a single level, the digital garden approach lets you go inwards. You can learn about each term and go deeper into each of its connections. The Glossary will show you each link that is related to the above interactive graph to it and all backlinks.\n\nThese will allow you to see connections in a visual way, that you would not otherwise.\n\nThis Glossary is forked from [Quartz](https://github.com/jackyzha0/quartz) and we thank Jacky for open-sourcing this gem.\n\n### Navigation\nYou can simply hit `ctrl/cmd+k` and **search** the whole Data Brain. Or you can click on the links and navigation through our content.\n\n### Interactive Graph\nUse the `Interactive Graph` on the bottom. It will appear every term. You can zoom and click on different nodes to navigate through the content.\n","lastmodified":"2022-09-09T09:31:21.28093937Z","tags":null},"/term/acid-transactions":{"title":"What are ACID Transactions?","content":"An ACID transaction secures that either all changes are successfully committed or rollbacked. It makes sure you never end in an inconsistent state. There is different concurrency control that, for example, guarantees consistency between reads and writes. Each [Data Lake Table Format](term/data%20lake%20table%20format.md) has other implementations and features here.","lastmodified":"2022-09-09T09:31:21.28093937Z","tags":null},"/term/airbyte-streams":{"title":"What are Airbyte Streams?","content":"In order to understand **AirbyteStreams**, let’s first talk about the **AirbyteCatalog**. An **AirbyteCatalog** describes the structure of data in a data source. It has a single field called streams that contains a list of **AirbyteStreams**. Each **AirbyteStream** contains a _name_ and _json_schema_ field. The _json_schema_ field describes the structure of a stream. This data model is intentionally flexible.\n\nIf we are using a data source that is a traditional relational database, each table in that database would map to an **AirbyteStream**. Each column in the table would be a key in the _properties_ field of the _json_schema_ field.\n\nIf we are using a data source that wraps an API with multiple different resources (e.g. _api/customers_ and _api/products_) each route would correspond to a stream.","lastmodified":"2022-09-09T09:31:21.28093937Z","tags":null},"/term/analytics":{"title":"What is Analytics?","content":"Analytics is the systematic computational analysis of [[data]] or statistics. It is used for the discovery, interpretation, and communication of meaningful patterns in data. It also entails applying data patterns toward effective decision-making.\n\nIt's highly related to [Business Intelligence](term/business%20intelligence.md).","lastmodified":"2022-09-09T09:31:21.28093937Z","tags":null},"/term/apache-arrow":{"title":"What is Apache Arrow?","content":"Apache Arrow is a development platform for in-memory analytics. It contains a set of technologies that enable big data systems to process and move data fast.\n\nRead more on [Data Lake File Format](term/data%20lake%20file%20format.md).","lastmodified":"2022-09-09T09:31:21.28093937Z","tags":null},"/term/apache-avro":{"title":"What is Apache Avro?","content":"Avro is an open-source data serialization system that helps with data exchange between systems, [programming languages](term/programming%20languages.md), and processing frameworks. Avro helps define a binary format for your data, as well as map it to the programming language of your choice.\n\nAvro has a JSON-like data model, but can be represented as either JSON or in a compact binary form. It comes with a **very sophisticated schema description language** that describes data. Avro is another [Data Lake File Format](term/data%20lake%20file%20format.md).\n\nRead more about how to build a Data Lake on top of it on our [Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2022-09-09T09:31:21.28093937Z","tags":null},"/term/apache-hadoop":{"title":"What is Apache Hadoop?","content":"Apache Hadoop is a collection of open-source software utilities that facilitates using a network of many computers to solve problems involving massive amounts of data and computation. It provides a software framework for distributed storage and processing of big data using the [MapReduce](term/map%20reduce.md) programming model.\n","lastmodified":"2022-09-09T09:31:21.28093937Z","tags":null},"/term/apache-hive":{"title":"What is Apache Hive?","content":"Apache Hive is a [Data Warehouse](term/data%20warehouse.md) software project built on top of [Apache Hadoop](term/apache%20hadoop.md) for providing data queries and analysis. Hive gives an SQL-like interface to query data stored in various databases and file systems that integrate with Hadoop. Traditional SQL queries must be implemented in the [MapReduce](term/map%20reduce.md) Java API to execute SQL applications and queries over distributed data. Hive provides the necessary SQL abstraction to integrate SQL-like queries ([HiveQL](https://en.wikipedia.org/wiki/Apache_Hive#HiveQL)) into the underlying Java without the need to implement queries in the low-level Java API.","lastmodified":"2022-09-09T09:31:21.28093937Z","tags":null},"/term/apache-hudi":{"title":"What is Apache Hudi?","content":"Apache Hudi is a [Data Lake Table Format](term/data%20lake%20table%20format.md) and was originally developed at Uber in 2016 (code-named and pronounced \"Hoodie\"), open-sourced end of 2016 ([first commit](https://github.com/apache/hudi/commit/0512da094bad2f3bcd2ddddc29e8abfec175dcfe) in 2016-12-16), and submitted to the Apache Incubator in January 2019. More about the back story on [The Apache Software Foundation Announces Apache® Hudi™ as a Top-Level Project](https://www.globenewswire.com/news-release/2020/06/04/2043732/0/en/The-Apache-Software-Foundation-Announces-Apache-Hudi-as-a-Top-Level-Project.html).\n\nRead more about how to build a Data Lake on top of it on our [Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2022-09-09T09:31:21.28093937Z","tags":null},"/term/apache-iceberg":{"title":"What is Apache Iceberg?","content":"Apache Iceberg is a [Data Lake Table Format](term/data%20lake%20table%20format.md) and was [initially developed](https://github.com/Netflix/iceberg) at Netflix to solve long-standing issues using huge, petabyte-scale tables. It was open-sourced in 2018 as an Apache Incubator project and graduated from the incubator on the 19th of May 2020. Their [first public commit](https://github.com/apache/iceberg/commit/a5eb3f6ba171ecfc517a4f09ae9654e7d8ae0291) was 2017-12-19—more insights about the story on [A Short Introduction to Apache Iceberg](https://medium.com/expedia-group-tech/a-short-introduction-to-apache-iceberg-d34f628b6799).\n\nRead more about how to build a Data Lake on top of it on our [Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2022-09-09T09:31:21.28093937Z","tags":null},"/term/apache-parquet":{"title":"What is Apache Parquet?","content":"Apache Parquet is a free and open-source column-oriented [Data Lake File Format](term/data%20lake%20file%20format.md) in the Apache Hadoop ecosystem. It is similar to RCFile and [ORC](term/orc.md), the other columnar-storage file formats in Hadoop, and is compatible with most of the data processing frameworks around Hadoop.\n\nRead more about how to build a Data Lake on top of it on our [Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2022-09-09T09:31:21.28093937Z","tags":null},"/term/business-intelligence":{"title":"What is Business Intelligence?","content":"Business intelligence (BI) leverages software and services to transform data into actionable insights that inform an organization’s business decisions. The new term is [Data Engineering](term/data%20engineering.md). The language of a BI engineer is [SQL](term/sql.md).\n\n## Goals of BI\nBI should produce a simple overview of your business, boost efficiency, and automate repetitive tasks across your organization. In more detail:\n  * **Roll-up capability** - (data) [Visualization](term/analytics.md) over the most important [KPIs][2] (aggregations) - like a cockpit in an airplane which gives you the important information at one glance.\n  * **Drill-down possibilities** - from the above high-level overview drill down the very details to figure out why something is not performing as planned. **Slice-and-dice or pivot your data from different angles.\n  * **Single source of truth** - instead of multiple spreadsheets or other tools with different numbers, the process is automated and done for all unified. Employees can talk about the business problem instead of the various numbers everyone has. Reporting, budgeting, and forecasting are automatically updated and consistent, accurate, and in timely manner.\n  * **Empower users**: With the so-called self-service BI, every user can analyze their data instead of only BI or IT persons.\n\nRead more on [Business Intelligence meets Data Engineering with Emerging Technologies](https://www.sspaeti.com/blog/business-intelligence-meets-data-engineering/).\n","lastmodified":"2022-09-09T09:31:21.28093937Z","tags":null},"/term/contribute-to-glossary":{"title":"How to Contribute to the Glossary","content":"\nIf you want to deploy direct, skip this and go directly to your preferred Deployment Methods:\n* Web edits through GitHub\n* Creating an Issue and we'll do the rest\n* Or cloning and running it locally\n\n## General Info\n[Quartz](https://quartz.jzhao.xyz) runs on top of [Hugo](https://gohugo.io/) so all notes are written in [Markdown](https://www.markdownguide.org/getting-started/) and can be edited through any editor.\n\n### Folder Structure\nThe content of the Glossary is in `content/term` folder. The rest outside of `content/` is the website/framework.\n\nTo edit the main home page, open `/content/_index.md`.\n### Links\nTo create a link between terms in the glossary, just create a normal link using Markdown pointing to the document in question. Please note that **all links should be relative to the root `/content` path**.\n```markdown\nFor example, I want to link this current document to `term/config.md`.\n[A link to the config page](term/config.md)\n```\n\nSimilarly, you can put local images anywhere in the `/content` folder.\n```markdown\nExample image (source is in content/images/example.png)\n![Example Image](/content/images/example.png)\n```\n\n### Lower Case\nTerms are lower case that links are also lowercase. When we create a link to a term, I usually capitalize the beginning of each word to make it look nice. E.g `[Apache Arrow](term/apache%20arrow.md)`. Other such as YAML I write all in capitals.\n\nWe didn't activate wikilinks, but that would be an option as well. See more on [editing](https://quartz.jzhao.xyz/notes/editing/).\n### Metatag with Front Matter\nHugo is picky when it comes to metadata for files. Make sure that your title is double-quoted and that you have a title defined at the top of your file like so. You can also add tags here as well.\n```yaml\n---\ntitle: \"What is a Glossary?\"\ntags:\n- example-tag\n- here i can add more we keep it lower case\nurl: \"term/my-other-domain\"\naliases:\n- Digital Garden\n- Second Brain\n---\n\nRest of your content here.\n```\n\n- `url`: this is not needed, only if the default link (name of the note) is not sufficient\n\t- all spaces will be replaced with `-` (dash).\n- `aliases`: Are like tags, you can add multiple and they will be linkable same as a adding a new term would be.\n\n\n## Web Edit with GitHub\nYou can either click on `Edit Source` on each page and directly edit on GitHub or you can create a [New Issue](https://github.com/airbytehq/glossary/issues).\n\n## Create an Issue\nIf you are unsure, you can always [create an issue](https://github.com/airbytehq/glossary/issues) on our GitHub repo and we will make sure to add the new entry.\n\n## Changing a lot? Clone locally\n### Clone it locally with git\nClone the [repo](https://github.com/airbytehq/glossary) with:\n```sh\ngit clone https://github.com/airbytehq/glossary.git\n```\n\n### Editors\n#### Obsidian as an Editor (Recommended)\nIf you want to use [Obsidian](https://obsidian.md/), which I recommend as it will handle all links when renaming terms, adding a nice Markdown view with lots of features (even if you don't need them) and showing backlinks and [graph](term/about%20this%20glossary.md#interactive-graph). Just open the Obsidian in the folder `content/`, there is a hidden folder called `.obsidian` which does the rest.\n\n1. ![](images/Pasted%20image%2020220908082431.png)\n2. ![](images/Pasted%20image%2020220908082439.png)\n\nMore details and step-by-step manual you see on [Quartz Setup](https://quartz.jzhao.xyz/notes/setup/), how to [Edit Notes ](https://quartz.jzhao.xyz/notes/editing/) and [How to set up Obsidian](https://quartz.jzhao.xyz/notes/obsidian/) (although the settings are already done when you open the `.obsidian` folder as described above).\n\n#### Use any other Editor\nOf course, as everything is Markdown, you can edit each file under `content/term` as a normal markdown file and publish (see below) changes to GitHub. Keep in mind, ethat very new term you create will automatically be created as a page in dthe eployment process or when you run it locally.\n\n### Preview Locally\n#### Setup\nQuartz runs on top of [Hugo](https://gohugo.io/) so all notes are written in [Markdown](https://www.markdownguide.org/getting-started/).\n\nWe need to install golang, hugo-obsidian and hugo. Follow the instructions on [Preview Changes on Quartz](https://quartz.jzhao.xyz/notes/preview-changes/).\n\n\u003e [!info]\n\u003e \n\u003e If you are running into an error saying that `command not found: hugo-obsidian`, make sure you set your `GOPATH` correctly! This will allow your terminal to correctly recognize hugo-obsidian as an executable.\n\nI added to my `~/.zshrc` (or `~/.bashrc`):\n ```sh\n#go path\nexport GOPATH=$HOME/go\nexport PATH=$PATH:$GOROOT/bin:$GOPATH/bin\n```\n#### Run it!\nAll you need to do it goint to your root directory of you cloned repo and start `make serve`:\n```sh\n# Navigate to your local Quartz folder\ncd \u003clocation-of-your-local-quartz\u003e\n\n# Start local server\nmake serve\n\n# View your site in a browser at http://localhost:1313/\n```\nThat's it, from now on that's how you run it. All changes you make will be automatically published, you do not need to stop and restart when you add terms, etc. (only the graph view will only be updated after stopping and serving again).\n\n## How to Publish\nCommit and Push to branch `hugo` and wait a couple of minutes until [GitHub Actions](https://github.com/airbytehq/glossary/actions) will deploy it automatically. At the moment we do not need to create PR's to make the updates as easy as possible. \n\nIf we encounter problems in the future, we might change that. If you are unsure, you can always [create an issue](https://github.com/airbytehq/glossary/issues) and we will make sure to add the new entry.\n","lastmodified":"2022-09-09T09:31:21.28093937Z","tags":null},"/term/cursor":{"title":"What is a Cursor?","content":"At a conceptual level, a cursor is a tracker that is used during [incremental synchronization](term/incremental%20synchronization.md) to ensure that only newly updated or inserted records are sent from a data source to a destination in any given synchronization iteration.\n\nAirbyte’s incremental synchronization can be conceptually thought of as a loop which periodically executes synchronization operations. Each iteration of this loop only replicates records that have been inserted or updated in the source system since the previous execution of this synchronization loop – in other words, each synchronization operation will copy only records that have not previously been replicated by previous synchronizations. This is much more efficient than copying an entire dataset on each iteration, which is the behavior of full refresh synchronization.\n\nSending only updated or newly inserted documents requires tracking which records have already been replicated in previous synchronizations. This is done by a cursor, which can be thought of as a pointer to the most recent record that has been replicated by a given synchronization. When selecting documents for synchronization, Airbyte includes the most recent cursor value as part of the query on the source system to ensure that only new/updated records will be replicated.\n\nFor example, a source database could contain records which include a field called `updated_at`, which stores the most recent time that a record is inserted or updated. If `updated_at` is selected as the cursor field, then after a given synchronization operation the cursor will remember the largest `updated_at` value that has been seen in the records that have been replicated to the destination in that synchronization. In the subsequent synchronization operation, records that have been inserted or updated on the source are retrieved by including the cursor value as part of the query, so that it only selects records where the `updated_at` value is greater than (and in some edge cases greater than or equal to) the largest `updated_at` value seen in the previous synchronization.\n\nNote that while it is not strictly necessary to choose a time field for a cursor field, the field that is chosen should be monotonically increasing over time.\n\nRead more on [Airbyte's Full Refresh Data Synchronization](https://airbyte.com/tutorials/full-data-synchronization).","lastmodified":"2022-09-09T09:31:21.28093937Z","tags":null},"/term/dag-directed-acyclic-graph":{"title":"What is a Directed Acyclic Graph (DAG)?","content":"A DAG is a graph where information must travel along with a finite set of nodes connected by vertices. There is no particular start or node and also no way for data to travel through the graph in a loop that circles back to the starting point.\n\nIt's a popular way of building data pipelines in the data engineering community as it clearly defines the [Data Lineage](term/data%20lineage.md). As well, it's made for a functional approach where you have the [idempotency](term/idempotency.md) to restart pipelines without side-effects","lastmodified":"2022-09-09T09:31:21.28093937Z","tags":null},"/term/data-asset":{"title":"What is a Data Asset?","content":"A data asset is typically a database table, a machine learning model, or a report. A persistent object that captures some understanding of the world. It's more a technical term where [Data Product](term/data%20product.md) is more used in general or in [Data Mesh](term/data%20mesh.md).","lastmodified":"2022-09-09T09:31:21.28093937Z","tags":null},"/term/data-catalog":{"title":"What is a Data Catalog?","content":"A Data Catalog is a centralized store where all your metadata data about your data is made searchable.\n\n**Think about a Google Search for your internal Metadata**. This is vital, as with [Data Lake](term/data%20lake.md) and other data stores, and you want the ability to search for your data. Data is growing exponentially, with 90% of the world’s data being generated alone in the last two years. It's hard to keep this amount over time. A data catalog solves the problem of the fast-growing handling of data internally.\n\nAn interesting read about the beginning of the Data Catalog is explained in the 2017 published paper about a [Data Context Service](http://cidrdb.org/cidr2017/papers/p111-hellerstein-cidr17.pdf). See as well the [Awesome Data Discovery and Observability](https://github.com/opendatadiscovery/awesome-data-catalogs) list on GitHub for an extensive list of existing tools.","lastmodified":"2022-09-09T09:31:21.28093937Z","tags":null},"/term/data-engineering":{"title":"What is Data Engineering?","content":"Data Engineering mainly helps overcome the bottlenecks of [Business Intelligence](term/business%20intelligence.md):\n- More transparency as tools are open-source mostly\n- More frequent data loads?\n- Extend additional [Machine Learning](term/machine%20learning.md) capabilities smoothly?\n\nIt's also the less famous sibling of data science.\n\nCompared to existing roles it would be a **software engineering plus business intelligence engineer including big data abilities** as the [Hadoop](term/apache%20hadoop.md) ecosystem, streaming, and computation at scale. Business creates more reporting artifacts themselves but with more data that needs to be collected, cleaned, and updated near real-time and complexity is expanding every day.\n\nWith that said more programmatic skills are needed similar to software engineering. **The emerging language at the moment is [Python](term/python.md)** while is used in engineering with tools alike [[Apache Airflow]], [Dagster](Dagster), etc. as well as data science with powerful libraries.\n\nAs a data engineer, you use mainly [SQL](term/sql.md) for almost everything except when using external data from an API. Here you'd use [ELT](term/elt.md) tools or write some [[data pipelines]] with the tools mentioned above.\n\nSee the video about the evolution of data engineers:\n{{\u003c youtube Si14Hgj4Lok \u003e}}\n","lastmodified":"2022-09-09T09:31:21.28093937Z","tags":null},"/term/data-engineering-concepts":{"title":"Data Engineering Concepts","content":"Some core concepts we are going to explore:\n\n- [Data Warehouse](term/data%20warehouse.md) vs [Data Lake](term/data%20lake.md) vs [Data Lakehouse](term/data%20lakehouse.md)\n- [Storage Layer](term/storage%20layer%20object%20store.md) vs [Data Lake File Format](term/data%20lake%20file%20format.md) vs [Data Lake Table Format](term/data%20lake%20table%20format.md)\n- [Modern Data Stack](term/modern%20data%20stack.md)\n- [Metrics Layer](term/metrics%20layer.md)\n- [ELT](term/elt.md) vs [ETL](term/etl.md)\n- [MapReduce](term/map%20reduce.md)\n- [Notebooks](term/notebooks.md)\n- [Dimensional Modeling](term/dimensional%20modeling.md)\n- [[OLTP]] vs [[OLAP]]\n- [[Data Modeling]]\n- [[Batch processing]] vs [[Streaming Processing]]\n- [[Indexing]]\n- [[Relational Database]] vs [[NoSQL Database]]","lastmodified":"2022-09-09T09:31:21.28093937Z","tags":null},"/term/data-engineering-guides":{"title":"Data Engineering Guides","content":"\nSome Data Engineering Guides that will help you learn [data engineering](term/data%20engineering.md):\n\n- **[Data Quality](https://airbyte.com/blog/data-quality-issues)**\n\t- How to handle [[Data Quality]] issues by detecting, understanding, fixing, and reduce\n- **[Data Lake / Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi)**\n\t- The what \u0026 why of a [Data Lake](term/data%20lake.md)\n\t- Differences between [Lakehouse](term/data%20lakehouse.md) \u0026 [Data Warehouse](term/data%20warehouse.md)\n\t- Components of a data lake\n\t\t1. [Storage Layer](term/storage%20layer%20object%20store.md)\n\t\t2. [Data Lake File Format](term/data%20lake%20file%20format.md)\n\t\t3. [Data Lake Table Format](term/data%20lake%20table%20format.md) with [Apache Parquet](term/apache%20parquet.md), [Apache Iceberg](term/apache%20iceberg.md), and [Apache Hudi](term/apache%20hudi.md)\n\t- Trends in the market\n\t- We answer questions such as:\n\t\t- How to build an open-source data lake offloading data for analytics?\n\t\t- How to [govern](term/data%20governance.md) your hundreds to thousands of files and have more database-like features?\n- **[Reverse ETL Explained](https://airbyte.com/blog/reverse-etl)**\n\t- A Brief Story of Data Integration: [ETL](term/etl.md) vs. [ELT](term/elt.md)\n\t- So, What is a [Reverse ETL](term/reverse%20etl.md)?\n\t- Technical Differences Between ETL and Reverse ETL\n\t- Typical Reverse ETL Use Cases\n\t- Reverse ETL and the [Data Hierarchy of Needs](term/data%20hierarchy%20of%20needs.md)\n- [Data Orchestration Trends](https://airbyte.com/blog/data-orchestration-trends)\n- [Data Integration Guide](https://airbyte.com/blog/data-integration)\n- [Understanding Change Data Capture (CDC)](https://airbyte.com/blog/change-data-capture-definition-methods-and-benefits)\n- [Using an ETL Framework vs Writing Yet Another ETL Script](https://airbyte.com/blog/etl-framework-vs-etl-script)\n\nSee more on [Data Insights Blog Posts](https://airbyte.com/blog-categories/data-insights).","lastmodified":"2022-09-09T09:31:21.28093937Z","tags":null},"/term/data-governance":{"title":"What is Data Governance?","content":"[**Data governance**](https://www.talend.com/resources/what-is-data-governance/) **is a collection of processes, roles, policies, standards, and metrics that ensure the effective and efficient use of information in enabling an organization to achieve its goals.** It establishes the processes and responsibilities that ensure the quality and security of the data used across a business or organization. Data governance defines who can take what action, upon what data, in what situations, and using what methods.\n\nRead more on our [Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2022-09-09T09:31:21.28093937Z","tags":null},"/term/data-hierarchy-of-needs":{"title":"The Data Hierarchy of Needs","content":"\nThe data hierarchy of needs in this image is inspired by [Grouparoo's blog post](https://www.grouparoo.com/blog/data-hierarchy-of-needs):\n![](images/Pasted%20image%2020220901183944.png)\n\nMore on [Reverse ETL Explained](https://airbyte.com/blog/reverse-etl#so-what-is-a-reverse-etl).\n","lastmodified":"2022-09-09T09:31:21.28093937Z","tags":null},"/term/data-lake":{"title":"What is a Data Lake?","content":"A Data Lake is a storage system with vast amounts of unstructured and structured data, stored as-is, without a specific purpose in mind, that can be built on multiple technologies such as Hadoop, NoSQL, Amazon Simple Storage Service, a relational database, or various combinations and different formats (e.g. Excel, CSV, Text, Logs, etc.).\n\nAccording to [Hortonworks Data Lake Whitepaper](http://hortonworks.com/wp-content/uploads/2014/05/TeradataHortonworks_Datalake_White-Paper_20140410.pdf), the data lake arose because new types of data needed to be captured and exploited by the enterprise. As this data became increasingly available, early adopters discovered that they could extract insight through new applications built to serve the business. The data lake supports the following capabilities:\n-   To capture and store raw data at scale for a low cost\n-   To store many types of data in the same repository\n-   To perform transformations on the data where the purpose may not be defined\n-   To perform new types of data processing\n-   To perform single-subject analytics based on particular use cases\n\nThe initial concept was created by Databricks in the [CIDR Paper](http://cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf) in 2021. Read more on our [Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/data-lake-file-format":{"title":"What is a Data Lake File Format?","content":"Data lake file formats are the new CSVs on the cloud. They are more column-oriented and compress large files with added features. The main players here are [Apache Parquet](term/apache%20parquet.md), [Apache Avro](term/apache%20avro.md), and [Apache Arrow](term/apache%20arrow.md). It’s the physical store with the actual files distributed around different buckets on your [Object Store](term/storage%20layer%20object%20store.md).\n\nYou can build more features with [Data Lake Table Format](term/data%20lake%20table%20format.md) on top. Read more on our [Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2022-09-09T09:31:21.28093937Z","tags":null},"/term/data-lake-table-format":{"title":"What is a Data Lake Table Format?","content":"Data lake table formats are very attractive as they are databases on [Data Lake](term/data%20lake.md). Same as a table, one **data lake table format bundles distributed files into one table that is otherwise hard to manage**. You can think of it as an abstraction layer between your physical data files and how they are structured to form a table.\n\nIt is built on top o the [Storage Layer](term/storage%20layer%20object%20store.md) and [Data Lake File Format](term/data%20lake%20file%20format.md). Read more on our [Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).\n","lastmodified":"2022-09-09T09:31:21.28093937Z","tags":null},"/term/data-lake-transaction-log":{"title":"What is a Data Lake Transaction Log?","content":"The **data lake transaction log** is the ordered record of every transaction since its inception. A transaction log is a common component used through many of its above-mentioned features, including [ACID Transactions](term/acid%20transactions.md), scalable metadata handling, and [Time Travel](term/time%20travel.md). For example, [Delta Lake](term/delta%20lake.md) creates a single [folder called `_delta_log`](https://airbyte.com/tutorials/load-data-into-delta-lake-on-databricks-lakehouse#step-5).","lastmodified":"2022-09-09T09:31:21.28093937Z","tags":null},"/term/data-lakehouse":{"title":"What is a Data Lakehouse?","content":"\nA Data Lakehouse open data management architecture that combines the flexibility, cost-efficiency, and scale of [Data Lake](term/data%20lake.md) with the data management and ACID transactions of [Data Warehouse](term/data%20warehouse.md) with Data Lake Table Formats (Delta Lake, Apache Iceberg \u0026 Hudi) that enable Business Intelligence (BI) and Machine Learning (ML) on all data.\n\nThe initial concept was created by Databricks in the [CIDR Paper](http://cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf) in 2021. Read more on our [Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/data-lineage":{"title":"What is Data Lineage?","content":"Data lineage uncovers the life cycle of data. It aims to show the complete data flow from start to finish. Data lineage is the process of understanding, recording, and visualizing data as it flows from data sources to consumption. This includes all data transformations (what changed and why).","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/data-mesh":{"title":"What is Data Mesh?","content":"The [Data Mesh Paper](https://martinfowler.com/articles/data-monolith-to-mesh.html) tries to eliminate silos between data teams, ensuring that the experience and knowledge about data are shared among all data consumers in the company. Data Mesh sees [Data As a product](term/data%20product.md). Data meshes are also about connecting platforms that those teams are using so data can be easily moved around for the organization's benefit. Companies will try to find better ways of unifying and connecting the tools so that data professionals don’t have to switch and work in a silo.\n\nData meshes try to eliminate the tensions between decentralizing and centralizing data resources, with some common infrastructure but otherwise mostly decentralized. It empowers data teams and gives ownership to domain experts.\n\nMore valuable resources such as a [short version](https://cnr.sh/essays/what-the-heck-data-mesh), a [visually appealing one](https://www.datamesh-architecture.com/)), or [applied in practice](https://youtu.be/eiUhV56uVUc).","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/data-ops":{"title":"What is DataOps?","content":"Similar to how [DevOps](term/dev%20ops.md) changed the way software is developed, DataOps is changing the way data products are created. With DataOps, data engineers and data scientists can work together, bringing a level of collaboration and communication, with a common goal of producing valuable insight for the business.","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/data-orchestrator":{"title":"What is a Data Orchestrator?","content":"A Data Orchestrator models dependencies between different tasks in [complex heterogeneous cloud environments](https://mattturck.com/data2021/) end-to-end. It handles integrations with legacy systems, new cloud-based tools, and your data lakes and data warehouses. It invokes [computation](https://en.wikipedia.org/wiki/Orchestration_(computing)), such as wrangling your business logic in [SQL](term/sql.md) and [Python](term/python.md) and applying ML models at the right time based on a time-based trigger or by custom-defined logic.\n\nMore Insights in [Data Orchestration Trends: The Shift from Data Pipelines to Data Products](https://airbyte.com/blog/data-orchestration-trends).","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/data-product":{"title":"What is a Data Product?","content":"[DJ Patil](https://twitter.com/dpatil), the former Chief Data Scientist of the United States, defined a data product as \"a product that facilitates an end goal through data.\" Also, [Data Mesh](term/data%20mesh.md) talks about \"data as a product.\" It applies more product thinking, whereas the \"Data Product\" essentially is a dashboard, report, and table in a [Data Warehouse](term/data%20warehouse.md) or a Machine Learning model. Sometimes Data Products are also called [Data Asset](term/data%20asset.md)s.","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/data-swamp":{"title":"What is a Data Swamp?","content":"Data swamps start to arise when there is a lack of responsibilities, data ownership, availability, and data governance.  It's when a [Data Lake](term/data%20lake.md) is unmanaged or unable to provide value. Sometimes a Data Swamp can also arise from a [Data Warehouse](term/data%20warehouse.md) due to existing hybrid models.","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/data-virtualization":{"title":"What is Data Virtualization?","content":"Data Virtualization helps you when you have many source systems from different technologies, but all of them are rather fast in response time, and if you don't run a lot of operational applications. In that way, you don't move and copy data around and pre-aggregate, but you have a [Semantic Layer](term/metrics%20layer.md) where you create your business models (like cubes), and only if you query this data virtualization layer does it query the data source. If you use, e.g. [Dremio](https://www.dremio.com/), there you use [Apache Arrow](term/apache%20arrow.md) technology which will cache and optimize a lot in-memory for you that you have as well as stonishing fast response times.","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/data-warehouse":{"title":"What is a Data Warehouse?","content":"A Data Warehouse, in short DWH, also known as an Enterprise Data Warehouse (EDW), is the traditional way of collecting data as we do [since 30+ years](https://tdwi.org/articles/2016/02/01/data-warehousing-30.aspx). The DWH serves to be the data integration from many different sources, the single point of truth and the data management, meaning cleaning, historizing, and data joined together. It provides greater executive insight into corporate performance with management Dashboards, Reports, or Ad-Hoc Analyses.\n\nVarious types of business data are analyzed with Data Warehouses. The need for it often becomes evident when analytic requirements run afoul of the ongoing performance of operational databases. Running a complex query on a database requires the database to enter a temporarily fixed state. It is often untenable for transactional databases. A data warehouse is employed to do the analytical work, leaving the transactional database free to focus on transactions.\n\nThe other characteristic is analyzing data from multiple origins (e.g., your Google Analytics with your CRM data). It is highly transformed and structured due to the ETL (Extract Transform Load) process.\n\nIf you wonder about the difference between a Data Warehouse, Data Lake, and a Lakehouse, read more on our [Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/delta-lake":{"title":"What is Delta Lake?","content":"Delta Lake is an open-source [Data Lake Table Format](term/data%20lake%20table%20format.md) project created by Databricks and kindly open-sourced with its [first public GitHub Commit](https://github.com/delta-io/delta/commit/14cb4e0267cc188e0fdd47e5b4f0235baf87874e) on 2019-04-22. Recently announced [Delta Lake 2.0](https://www.databricks.com/blog/2022/06/30/open-sourcing-all-of-delta-lake.html).\n\nRead more about how to build a Data Lake on top of it on our [Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi) or if you are curious to build a Delta Lake destination with Airbyte [Load Data into Delta Lake on Databricks Lakehouse](https://airbyte.com/tutorials/load-data-into-delta-lake-on-databricks-lakehouse).","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/dev-ops":{"title":"What is DevOps?","content":"DevOps is a combination of software developers (dev) and operations (ops). It is defined as a software engineering methodology that aims to integrate the work of software development and software operations teams by facilitating a culture of collaboration and shared responsibility.\n\nIs also related to [DataOps](term/data%20ops.md)","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/dimensional-modeling":{"title":"What is Dimensional Modeling?","content":"Dimensional modeling (DM) is part of the Business Dimensional Lifecycle methodology developed by [[Ralph Kimball]], which includes a set of methods, techniques, and concepts for use in [Data Warehouse](term/data%20warehouse.md) design.\n\nAs a bottom-up approach, the approach focuses on identifying the critical business processes within a business and modeling and implementing these before adding additional business processes.  An alternative approach from [[Bill Inmon]] advocates a top-down design of the model of all the enterprise data using tools such as Entity-Relationship Modeling (ER).\n\nRead more on [Data Modeling with SQL and dbt](https://airbyte.com/blog/sql-data-modeling-with-dbt).\n","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/elt":{"title":"What is ELT?","content":"Compared to [ETL](term/etl.md), in an ELT (extract load and transform) pipeline, data enrichment and transformation occur inside the [Data Warehouse](term/data%20warehouse.md), not in a processing server like in the case of ETL pipelines. The shift has been primarily made possible thanks to the appearance of cloud-based data warehouses like Redshift, BigQuery, or Snowflake.\n\nIt is also tightly connected with [Reverse ETL](term/reverse%20etl.md). And you can read more on [Reverse ETL Explained](https://airbyte.com/blog/reverse-etl#so-what-is-a-reverse-etl) or [Airbyte.com](https://airbyte.com).","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/etl":{"title":"What is ETL?","content":"ETL (extract transform and load) is loading data in a three-phase process and is a classic paradigm that emerged in the 1970s. Recently, it has shifted to [ELT](term/elt.md). The ELT philosophy dictates that data should be untouched – apart from minor cleaning and filtering – as it moves through the extraction and loading stages so that the raw data is always accessible in the [Data Warehouse](term/data%20warehouse.md).\n\n## ETL is Changing\nThe way we do ETL is changing. For a long time it was done with ETL tools such as Informatica, IBM Datastage, Cognos, AbInitio, or Microsoft SSIS, today we use more programmatic or configuration-driven platforms like [[Airflow]], [[Dagster]], and [[Temporal]]. And with data needs grow, and the need for having data faster, the trend shifted to ELT.\n\n## ETL vs ELT\nETL (Extract Transform and Load) and ELT (Extract Load and Transform). ETL was originally used for [Data Warehousing](term/data%20warehouse.md) and ELT for creating a [Data Lake](term/data%20lake.md). The key difference is that the data schema and the transformation need to be done before the data lands at the destination.\n\nMore on [Airbyte.com](https://airbyte.com) or [Reverse ETL Explained](https://airbyte.com/blog/reverse-etl#so-what-is-a-reverse-etl).","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/functional-data-engineering":{"title":"What is Functional Data Engineering?","content":"Functional Data Engineering brings _clarity_. When functions are \"pure,\" they do not have side effects. They can be written, tested, reasoned about, and debugged in isolation without understanding the external context or history of events surrounding their execution. Its [Functional Programming](term/functional%20programming.md) applied to the field of data engineering initiated by [Maxime Beauchemin](term/maxime%20beauchemin.md) with [Functional Data Engineering — a modern paradigm for batch data processing](https://maximebeauchemin.medium.com/functional-data-engineering-a-modern-paradigm-for-batch-data-processing-2327ec32c42a).\n","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/functional-programming":{"title":"What is Functional Programming?","content":"\nFunctional Programming is a style of building functions that threaten computation as a mathematical function that avoids changing state and mutable data. It is a declarative programming paradigm, which means programming expressive and [declarative](https://airbyte.com/blog/data-orchestration-trends) as opposed to imperative. It's getting more popular with the rise of [Functional Data Engineering](term/functional%20data%20engineering.md).\n\nSee also [Programming Languages](term/programming%20languages.md).","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/idempotency":{"title":"What is Idempotency?","content":"Idempotency is the property of a particular operation that can be applied multiple times without changing the resulting outcome by being given the same inputs. It is used in [Functional Programming](term/functional%20programming.md) and was the foundation for [Functional Data Engineering](term/functional%20data%20engineering.md).","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/incremental-synchronization":{"title":"What is Incremental Synchronization?","content":"Incremental synchronization is a process which efficiently copies data to a destination system by periodically executing queries on a source system for records that have been updated or inserted since the previous sync operation. Only those records that have been recently inserted or updated will be sent to the destination, which is much more efficient than copying an entire data set on each sync operation. Incremental synchronization makes use of a cursor field such as `updated_at` (or whatever you wish to call the field) to determine which records should be propagated, and only records with an `updated_at` value that is newer than the `updated_at` value of the most recent record sent in the previous sync should be replicated.\n\nHowever, without special consideration, records that have been deleted in the source system will not be propagated to the destination as they will never appear in the results from such a query. This may be addressed by [Soft Deletes](term/soft%20delete.md) or by making use of [CDC replication](https://airbyte.com/blog/change-data-capture-definition-methods-and-benefits).","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/kubernetes":{"title":"What is Kubernetes?","content":"It’s a platform that allows you to run and orchestrate container workloads. [**Kubernetes**](https://stackoverflow.blog/2020/05/29/why-kubernetes-getting-so-popular/) **has become the de-facto standard** for your cloud-native apps to (auto-) [scale-out](https://stackoverflow.com/a/11715598/5246670) and deploy your open-source zoo fast, cloud-provider-independent. No lock-in here. Kubernetes is the **move from infrastructure as code** towards **infrastructure as data**, specifically as [YAML](term/yaml.md). With Kubernetes, developers can quickly write applications that run across multiple operating environments. Costs can be reduced by scaling down.","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/lambda-architecture":{"title":"What is a Lambda Architecture?","content":"Lambda architecture is a data-processing architecture designed to handle massive quantities of data by taking advantage of both batch and stream-processing methods. This approach to architecture attempts to balance latency, throughput, and fault tolerance using batch processing to provide comprehensive and accurate views of batch data, while simultaneously using real-time stream processing to provide views of online data. The two view outputs may be joined before the presentation. The rise of lambda architecture is correlated with the growth of big data, real-time analytics, and the drive to mitigate the latencies of [MapReduce](term/map%20reduce.md).","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/machine-learning":{"title":"What is Machine Learning?","content":"Machine learning (ML) is a type of artificial intelligence (AI) that allows software applications to become more accurate at predicting outcomes without being explicitly programmed to do so. Machine learning [algorithms](https://www.techtarget.com/whatis/definition/algorithm) use historical data as input to predict new output values.\n\nMore on [Machine Learning  | Tech Target](https://www.techtarget.com/searchenterpriseai/definition/machine-learning-ML).","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/map-reduce":{"title":"What is MapReduce?","content":"MapReduce is a programming paradigm that enables massive scalability across hundreds or thousands of servers in a Hadoop cluster. As the processing component, MapReduce is the heart of [Apache Hadoop](term/apache%20hadoop.md). The term \"MapReduce\" refers to two separate and distinct tasks that Apache Hadoop programs perform.","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/master-data-management-mdm":{"title":"What is Master Data Management (MDM)?","content":"Master data management is a method to centralize master data. It's the bridge between the business that maintain the data and know them best and the data folks, and it's a tool of choice. It helps with uniformity, accuracy, stewardship, semantic consistency, and accountability of mostly enterprise master [Data Assets](term/data%20asset.md).","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/maxime-beauchemin":{"title":"Maxime Beauchemin","content":"Creator of [[Apache Airflow]] and [Apache Superset](term/apache%20superset.md).\n\nStarted as [Business Intelligence](term/business%20intelligence.md) Engineer and is working now at [[Preset]] (Superset as a Service).\n\nStarter of [idempotency](term/idempotency.md) and [functional data engineering](term/functional%20data%20engineering.md).\n","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/metrics-layer":{"title":"What is a Metrics Layer?","content":"A Metrics Layer also called Headless BI or sometimes Semantic Layer includes a specification of metrics such as measures and dimensions. Additionally, it can contain model parsing from files (mostly [YAML](term/yaml.md) and APIs to create and execute metric logic; some include a cache layer. A Metrics Layers encourages us to enforce the [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself) (Do not repeat yourself) principle by defining it once and population it to any BI tools used or integrated into internal applications or processes\n\n‍","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/modern-data-stack":{"title":"What is Modern Data Stack?","content":"The Modern Data Stack (MDS) is a heap of open-source tools to achieve end-to-end analytics from ingestion to transformation to ML over to a columnar data warehouse or lake solution with an analytics BI dashboard backend. This stack is extendable in any way needed, such as data quality, [Data Catalog](term/data%20catalog.md), etc.\n\nThe goal of an MDS is to get data insight with the best suitable tools for each part. It's noteworthy that it's a relatively new term and not yet 100% agreed on.\n","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/notebooks":{"title":"What are Notebooks?","content":"\nData notebooks popularized by Jupyter notebooks are the centralized IDE inside a browser for doing collaborative work.\n\n1. Notebooks that are popularized and in heavy use today.\n\t- [Jupyter Notebook](https://jupyter.org/) and [JupyterHub](https://jupyter.org/hub)\n\t\t- Automation on top of Jupyter notebooks: [Naas](https://github.com/jupyter-naas/awesome-notebooks)\n\t- [Zeppelin](https://zeppelin.apache.org/)\n\t- [Databricks Notebook](https://docs.databricks.com/notebooks/index.html)\n2. Newer versions of Jupyter notebooks with more integrated features and an integrated cloud\n\t- [HEX](https://hex.tech/)\n\t- [Deepnote](https://deepnote.com/)\n","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/orc":{"title":"What is ORC?","content":"The **Optimized Row Columnar** (ORC) [Data Lake File Format](term/data%20lake%20file%20format.md) provides a highly efficient way to store Hive data. It was designed to overcome the limitations of the other Hive file formats. Using ORC files improves performance when Hive is reading, writing, and processing data.\n\nRead more about how to build a Data Lake on top of it on our [Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).\n","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/programming-languages":{"title":"Programming Languages?","content":"2022 marks JavaScript’s tenth year in a row as the most commonly used programming language according to [Stack Overflow Developer Survey 2022](https://survey.stackoverflow.co/2022/#section-most-popular-technologies-programming-scripting-and-markup-languages). Further, they say: People learning to code are more likely than Professional Developers to report using Python (58% vs 44%), C++ (35% vs 20%), and C (32% vs 17%).\n\nProgramming Languages (so far):\n- [SQL](term/sql.md)\n- [Python](term/python.md)\n\nSee also [Functional Programming](term/functional%20programming.md) or [Functional Data Engineering](term/functional%20data%20engineering.md).","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/python":{"title":"What is Python?","content":"Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically-typed and garbage-collected. It supports multiple programming paradigms, including structured, object-oriented, and [Functional Programming](term/functional%20programming.md).\n\nPython is the de facto standard for [Data Engineering](term/data%20engineering.md) next to [SQL](term/sql.md). If you want to learn Python, see the Freecodecamp Python Course in under 300 hours:\n{{\u003c youtube vMl4YUch7x4 \u003e}}\n","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/reverse-etl":{"title":"What is Reverse ETL?","content":"Reverse ETL is the flip side of the [ETL](term/etl.md)/[ELT](term/elt.md). **With Reverse ETL, the data warehouse becomes the source rather than the destination**. Data is taken from the warehouse, transformed to match the destination's data formatting requirements, and loaded into an application – for example, a CRM like Salesforce – to enable action.\n\nIn a way, the Reverse ETL concept is not new to data engineers, who have been enabling data movement warehouses to business applications for a long time. As [Maxime Beauchemin](term/maxime%20beauchemin.md) mentions in [his article](https://preset.io/blog/reshaping-data-engineering/), Reverse ETL “appears to be a modern new means of addressing a subset of what was formerly known as  [Master Data Management (MDM)](term/master%20data%20management%20(mdm).md).”\n\nRead more about in [Reverse ETL Explained](https://airbyte.com/blog/reverse-etl#so-what-is-a-reverse-etl).","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/schema-evolution":{"title":"What is Schema Evolution?","content":"Automatic Schema Evolution is a crucial feature in [Data Lake Table Format](term/data%20lake%20table%20format.md)s as changing formats is still a pain in today's data engineer work. Schema Evolution means adding new columns without breaking anything or even enlarging some types. You can even rename or reorder columns, although that might break backward compatibilities. Still, we can change one table, and the table format takes care of switching it on all distributed files. Best of all does not require e rewrite of your table and underlying files.\n\nSee also [ACID Transactions](term/acid%20transactions.md).","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/slowly-changing-dimension-scd":{"title":"What is Slowly Changing Dimension?","content":"A Slowly Changing Dimension (SCD) is **a dimension that stores and manages both current and historical data over time in a [Data Warehouse](term/data%20warehouse.md)**. It is considered and implemented as one of the most critical ETL tasks in tracking the history of dimension records.","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/soft-delete":{"title":"What is a Soft Delete?","content":"In order to propagate records that have been deleted when using [Incremental Synchronization](term/incremental%20synchronization.md) modes, records in a database may include a field that indicates that a record should be treated as if it has been removed. This is necessary because incremental synchronization does not replicate documents that are fully deleted from a source system.\n\nFor example, a boolean flag such as `is_deleted` could be used to indicate that a record should be treated as if it has been deleted. All queries would need to be written so as to exclude records/documents where `is_deleted` is set, and periodically executed background jobs can be used to remove all documents where `is_deleted` is set.\n\n‍\n","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/sql":{"title":"What is SQL?","content":"SQL is **a standardized language used to interact with relational [[databases]]**. It stands for structured query language (SQL) and defines a standard [programming language](term/programming%20languages.md) utilized to extract, organize, manage, and manipulate data stored in relational databases.\n\nHere are different levels you can go into ([Source](https://twitter.com/largedatabank/status/1559651463919452161)):\n![](images/Pasted%20image%2020220901182014.png)\n\nSee more on [SQL-Levels Explained](https://github.com/airbytehq/SQL-Levels-Explained).","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/storage-layer-object-store":{"title":"What is a Storage Layer / Object Store?","content":"A storage layer or object storage are services from the three big cloud providers, AWS S3, Azure Blob Storage, and Google Cloud Storage. The web user interface is easy to use. **Its features are very basic, where, in fact, these object stores store distributed files exceptionally well.** They are also highly configurable, with solid security and reliability built-in.\n\nYou can build on with  [Data Lake File Format](term/data%20lake%20file%20format.md) or [Data Lake Table Format](term/data%20lake%20table%20format.md). Read more on our [Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/time-travel":{"title":"What is Time Travel?","content":"With time travel, the [Data Lake Table Format](term/data%20lake%20table%20format.md) versions the big data you store in your [Data Lake](term/data%20lake.md). You can access any historical version of that data, simplifying data management with easy-to-audit, rollback data in case of accidental bad writes or deletes, and reproduce experiments and reports. Time travel enables reproducible queries as you can query two different versions simultaneously.\n\nRead more about how to build a Data Lake on top of it on our [Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null},"/term/yaml":{"title":"What is YAML?","content":"YAML is a data serialization language often used to write configuration files. Depending on whom you ask, YAML stands for yet another markup language, or YAML isn’t markup language (a recursive acronym), which emphasizes that YAML is for data, not documents.","lastmodified":"2022-09-09T09:31:21.284939434Z","tags":null}}